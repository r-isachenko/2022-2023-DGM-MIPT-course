{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework6: VAE limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Theory (4pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Discrete vs continuous model (2pt)\n",
    "Let's suppose we have 2 generative models for images of size $W \\times H \\times C$, where $W$ - image width, $H$ - image height, $C$ - number of channels. \n",
    "\n",
    "* The first model $p_1(\\mathbf{x} | \\boldsymbol{\\theta})$ outputs a discrete distribution for each pixel  $\\text{Categorical}(\\boldsymbol{\\pi})$, где $\\boldsymbol{\\pi} = (\\pi_1, \\dots,  \\pi_{256})$. \n",
    "\n",
    "* The second model $p_2(\\mathbf{x} | \\boldsymbol{\\theta})$ models a discrete distribution by a continuous mixture of logistic functions ($\\boldsymbol{\\pi}$ - mixing distribution):\n",
    "$$\n",
    "    P(x | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\pi}) = P(x + 0.5 | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\pi}) - P(x - 0.5 | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\pi}).\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p(\\nu | \\boldsymbol{\\mu}, \\mathbf{s}, \\boldsymbol{\\pi}) = \\sum_{k=1}^K \\pi_k p(\\nu | \\mu_k, s_k).\n",
    "$$\n",
    "\n",
    "Each of the models outputs parameters of pixel distributions.\n",
    "\n",
    "* Calculate the dimensions of the output tensor for the model $p_1(\\mathbf{x} | \\boldsymbol{\\theta})$ and for the model $p_2(\\mathbf{x} | \\boldsymbol{\\theta})$. \n",
    "* At what number of mixture components $K$ is the number of elements of the output tensor for $p_2(\\mathbf{x} | \\boldsymbol{\\theta})$ becomes greater than $p_1(\\mathbf{x} | \\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfuMscdgelAu"
   },
   "source": [
    "### Problem 2: ELBO surgery (2pt)\n",
    "\n",
    "In lecture 7 we proved the [ELBO surgery](http://approximateinference.org/accepted/HoffmanJohnson2016.pdf) theorem:\n",
    "$$\n",
    "    \\frac{1}{n} \\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || p(\\mathbf{z})) = KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z})) + \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}],\n",
    "$$\n",
    "where the first term is $KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z}))$ includes the aggregated posterior distribution $q_{\\text{agg}}(\\mathbf{z})$ and the prior distribution $p(\\mathbf{z})$. Our goal now is to deal with the second term. At the lecture, the second term was equal to:\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\frac{1}{n}\\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || q_{\\text{agg}}(\\mathbf{z})).\n",
    "$$\n",
    "In fact, this is a mutual information between $\\mathbf{x}$ and $\\mathbf{z}$ on the empirical distribution of data and the distribution of $q(\\mathbf{z} | \\mathbf{x})$. Let treat the index of the sample $i$ as a random variable.\n",
    "$$\n",
    "    q(i, \\mathbf{z}) = q(i) q(\\mathbf{z} | i); \\quad p(i, \\mathbf{z}) = p(i) p(\\mathbf{z}); \\quad \n",
    "    q(i) = p(i) = \\frac{1}{n}.\n",
    "$$\n",
    "$$\n",
    "    \\quad q(\\mathbf{z} | i) = q(\\mathbf{z} | \\mathbf{x}_i) \\quad q_{\\text{agg}}(\\mathbf{z}) = \\sum_{i=1}^n q(i, \\mathbf{z}) = \\frac{1}{n} \\sum_{i=1}^n q(\\mathbf{z} | \\mathbf{x}_i);  \n",
    "$$\n",
    "Mutual information is a measure of independence between two random variables.\n",
    "$$\n",
    "\t\\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\mathbb{E}_{q(i, \\mathbf{z})} \\log \\frac{q(i, \\mathbf{z})}{q(i)q_{\\text{agg}}(\\mathbf{z})}.\n",
    "$$\n",
    "Prove that 2 expressions for mutual information are equal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtRUeZWJw4VY"
   },
   "source": [
    "```your solution```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vK5A_F3z9or",
    "outputId": "d3583de1-d7b7-4fd7-e339-ececbb343de4"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"2022-2023-DGM-MIPT-course\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/r-isachenko/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\n",
    "!pip install ./{REPO_NAME}/homeworks/\n",
    "!rm -Rf {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDjpZD6ez_TT"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, plot_training_curves\n",
    "from dgm_utils import show_samples, visualize_images, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieenT6NCp_OK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZZ-ef7XkQiT"
   },
   "source": [
    "# Task 2: VAE with Autoregressive decoder on MNIST (4pt)\n",
    "\n",
    "In this task you will fit the VAE model with [autoregressive decoder](https://arxiv.org/abs/1611.05013) to the MNIST dataset. We discussed this topic at Lecture 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "d7SDW4JHgKFr",
    "outputId": "0ebbea6e-a510-4427-c267-e631092fde24"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset('mnist', flatten=False, binarize=True)\n",
    "visualize_images(train_data, 'MNIST samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npF9ccu90sGV"
   },
   "source": [
    "First of all, let implement the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQ6UV0Fb1AeO"
   },
   "outputs": [],
   "source": [
    "def get_normal_KL(mean_1, log_std_1, mean_2=None, log_std_2=None):\n",
    "    \"\"\"\n",
    "        This function should return the value of KL(p1 || p2),\n",
    "        where p1 = Normal(mean_1, exp(log_std_1)), p2 = Normal(mean_2, exp(log_std_2) ** 2).\n",
    "        If mean_2 and log_std_2 are None values, we will use standart normal distribution.\n",
    "        Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    if mean_2 is None:\n",
    "        mean_2 = torch.zeros_like(mean_1)\n",
    "    if log_std_2 is None:\n",
    "        log_std_2 = torch.zeros_like(log_std_1)\n",
    "    # ====\n",
    "    # your code\n",
    "\n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_KL():\n",
    "    assert np.isclose(get_normal_KL(torch.tensor(2), torch.tensor(3), torch.tensor(0), torch.tensor(0)).numpy(), 200.2144, rtol=1e-3)\n",
    "    assert np.isclose(get_normal_KL(torch.tensor(2), torch.tensor(3), torch.tensor(4), torch.tensor(5)).numpy(), 1.50925, rtol=1e-3)\n",
    "    assert np.allclose(get_normal_KL(torch.tensor((10, 10)), torch.tensor((2, 4)), torch.tensor((3, 5))).numpy(), [49.2990, 1498.479], rtol=1e-3)\n",
    "\n",
    "test_KL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWj9VtqVSgNn"
   },
   "source": [
    "We will use PixelCNN model as the VAE decoder.\n",
    "\n",
    "First of all we need implement masked convolution 2d layer for autoregressive decoder. It is totally the same as the layer that we used in homework 1.\n",
    "\n",
    "However, there is a slight difference. Here, we need to model the distribution \n",
    "$$\n",
    "    p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\prod_{j=1}^m p(x_j | \\mathbf{x}_{1:j-1}, \\mathbf{z}, \\boldsymbol{\\theta}).\n",
    "$$\n",
    "Each conditional $p(x_j | \\mathbf{x}_{1:j-1}, \\mathbf{z}, \\boldsymbol{\\theta})$ is conditioned on the latent variable $\\mathbf{z}$.\n",
    "\n",
    "To implement this conditioning we will pass $\\mathbf{z}$ to our MaskedConv2d. There we will apply Linear layer to $\\mathbf{z}$ and then add it to the output of masked convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iazovzdJpc2Q"
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size=5, padding=0, conditional_size=None):\n",
    "        assert mask_type in ['A', 'B']\n",
    "        super().__init__(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "        if conditional_size is not None:\n",
    "            self.cond_op = nn.Linear(conditional_size, self.out_channels)\n",
    "\n",
    "    def forward(self, input, cond=None):\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply masked convolution and get \"out\" variable\n",
    "\n",
    "        # ====\n",
    "\n",
    "        if cond is not None:\n",
    "            cond = self.cond_op(cond)\n",
    "            out = out + cond.view(cond.shape[0], self.out_channels, 1, 1)\n",
    "        return out\n",
    "\n",
    "    def create_mask(self, mask_type):\n",
    "        # ====\n",
    "        # your code\n",
    "        # do not forget about mask_type\n",
    "\n",
    "        # ====\n",
    "\n",
    "\n",
    "def test_masked_conv2d():\n",
    "    layer = MaskedConv2d('A', 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.zeros((2, 2)))\n",
    "\n",
    "    layer = MaskedConv2d('B', 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.ones((2, 2)))\n",
    "\n",
    "\n",
    "test_masked_conv2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb0FcBizpYLO"
   },
   "source": [
    "[Layer Normalization](https://arxiv.org/abs/1607.06450) helps to stabilize training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0IbeNTGpWIx"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__(n_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        x = super().forward(x)\n",
    "        return x.permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJgA4lmFz2oZ"
   },
   "source": [
    "Let implement PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4J1G9LDCerC"
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_shape, \n",
    "        n_filters=256, \n",
    "        kernel_size=3, \n",
    "        n_layers=7,\n",
    "        use_layer_norm=True,\n",
    "        conditional_size=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # ====\n",
    "        # your code\n",
    "        # apply the sequence of MaskedConv2d -> LayerNorm -> ReLU\n",
    "        # note that the first conv layer should be of type 'A'\n",
    "        # the last layer should be MaskedConv2d\n",
    "        # define self.net as list of layers\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        out = (x.float() - 0.5) * 2\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, MaskedConv2d):\n",
    "                out = layer(out, cond=cond)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        return out.view(x.shape[0], 2, 1, *self.input_shape)\n",
    "\n",
    "    def loss(self, x, cond=None):\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def sample(self, n, cond=None):\n",
    "        # read carefully the sampling process\n",
    "        samples = torch.zeros(n, 1, *self.input_shape).cuda()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[0]):\n",
    "                for c in range(self.input_shape[1]):\n",
    "                    logits = self(samples, cond=cond)[:, :, :, r, c]\n",
    "                    probs = F.softmax(logits, dim=1).squeeze(-1)\n",
    "                    samples[:, 0, r, c] = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "        return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r7XFtAU0DXF"
   },
   "source": [
    "We will use simple convolutional encoder here. Look carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBBDv4skz9uE"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "        )\n",
    "        conv_out_dim = input_shape[0] // 4 * input_shape[1] // 4 * 64\n",
    "        self.fc = nn.Linear(conv_out_dim, 2 * latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x.float() - 0.5) * 2\n",
    "        out = self.convs(x)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        mu, log_std = self.fc(out).chunk(2, dim=1)\n",
    "        return mu, log_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbtrVLT10LYG"
   },
   "source": [
    "Now it is time to implement the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5EoNfoW0BcM"
   },
   "outputs": [],
   "source": [
    "class ARDecoderVAE(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent, free_bits=None):\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 2\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        self.free_bits = free_bits\n",
    "        self.encoder = ConvEncoder(input_shape, n_latent)\n",
    "        self.decoder = PixelCNN(\n",
    "            input_shape, \n",
    "            n_filters=32, \n",
    "            n_layers=3,\n",
    "            kernel_size=7, \n",
    "            conditional_size=n_latent\n",
    "        )\n",
    "\n",
    "    def prior(self, n):\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standart normal for prior)\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def loss(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) apply reparametrization trick\n",
    "        # 3) get decoder loss (reconstruction loss)\n",
    "        # 4) get kl loss using get_normal_KL\n",
    "        # 5) apply free_bits\n",
    "\n",
    "        # ====\n",
    "\n",
    "        return {\n",
    "            'total_loss': recon_loss + kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            z = self.prior(n).cuda()\n",
    "            samples = self.decoder.sample(n, cond=z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318,
     "referenced_widgets": [
      "81198526b98c42d793428a4d14401039",
      "64d325abc5c545e1920ec04548bb8290",
      "205f8fd5704f4750a1fdf33d1cedd554",
      "ea966af7e6764aa8bd4c7e1c9c338632",
      "0a72ddd90c004cb7a3f0c646beccc0f3",
      "b01cb1e3afa14947bb1702bfa401cf15",
      "153399d43c2b4830951d3bf285d06f70",
      "0f7285aa986840f6a5073f6534eb2947",
      "5e137db2f7b546e9a327b9f34969fa38",
      "e75f80b28069407aa75b170fe863dc0b",
      "d61cc7e765d6485d85647d25ef4d8117"
     ]
    },
    "id": "i4IgkdDuCetl",
    "outputId": "25f5b1d9-47a2-4b21-cae6-ab0a5be1cfb7"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE =  # any adequate value\n",
    "EPOCHS =      # < 10 \n",
    "LR =          # < 1e-2\n",
    "FREE_BITS =   # < 10\n",
    "# ====\n",
    "\n",
    "train_data, test_data = load_dataset('mnist', binarize=True)\n",
    "\n",
    "model = ARDecoderVAE(input_shape=(28, 28), free_bits=FREE_BITS, n_latent=16)\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, use_cuda=USE_CUDA, use_tqdm=True, lr=LR)\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-yGI6iF17oB"
   },
   "source": [
    "Note that sampling from our model is sequential now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "id": "E_X3IGjWCew0",
    "outputId": "e3d7a783-efe2-448e-e333-dc117cd2b117"
   },
   "outputs": [],
   "source": [
    "samples = model.sample(100)\n",
    "\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(2 * x - 1)\n",
    "    x_recon = model.decoder.sample(50, cond=z)\n",
    "x = x.cpu().numpy()\n",
    "reconstructions = np.concatenate((x, x_recon), axis=0)\n",
    "\n",
    "samples = samples.astype('float32')\n",
    "reconstructions = reconstructions.astype('float32')\n",
    "\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "show_samples(samples, title='Samples')\n",
    "show_samples(reconstructions, title='Reconstructions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HLQR8FikQd_"
   },
   "source": [
    "# Task 3: VAE with Autoregressive flow-based prior on CIFAR10 (5pt)\n",
    "\n",
    "In this task you will fit the VAE model with [flow-based prior](https://arxiv.org/abs/1611.02731) to the CIFAR10 dataset. We discussed this topic at Lecture 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "dTW5lhwv-J9w",
    "outputId": "6617feb7-3135-4716-ac1c-845327745ceb"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset('cifar10')\n",
    "visualize_images(train_data, 'CIFAR10 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMNE7poKkqio"
   },
   "source": [
    "The model consists of:\n",
    "* convolutional encoder (variational posterior destrituion $q(\\mathbf{z} | \\mathbf{x})$);\n",
    "* convolutional decoder (generative distribution $p(\\mathbf{x} | \\mathbf{z})$);\n",
    "* autoregressive prior.\n",
    "\n",
    "We will use MADE model as autoregressive prior. MADE Autoregressive frow (mapping from $\\mathbf{z}\\rightarrow \\boldsymbol{\\epsilon}$) should output location $\\mu_{\\boldsymbol{\\lambda}}(\\mathbf{z})$ and scale parameters $\\sigma_\\boldsymbol{\\lambda}(\\mathbf{z})$. The mapping from $\\mathbf{z}$ to $\\boldsymbol{\\epsilon}$ has the form:\n",
    "$$\n",
    "    \\boldsymbol{\\epsilon} = f(\\mathbf{z}, \\boldsymbol{\\lambda}) = \\mathbf{z} * \\sigma_\\boldsymbol{\\lambda}(\\mathbf{z}) + \\mu_{\\boldsymbol{\\lambda}}(\\mathbf{z}).\n",
    "$$\n",
    "\n",
    "The ELBO objective in this task is:\n",
    "$$\n",
    "    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}, \\boldsymbol{\\lambda}) = E_{q(\\mathbf{z}|\\mathbf{x}, \\boldsymbol{\\phi})}[\\log{p(\\mathbf{x}|\\mathbf{z}, \\boldsymbol{\\theta})}] - E_{q(\\mathbf{z}|\\mathbf{x}, \\boldsymbol{\\phi})}[\\log{q(\\mathbf{z}|\\mathbf{x}, \\boldsymbol{\\phi})} - \\log{p(\\mathbf{z} | \\boldsymbol{\\lambda})}]\n",
    "$$\n",
    "where the logarithm of prior distribution is given by chage of variable (CoV) theorem\n",
    "$$\n",
    "    \\log{p(\\mathbf{z} | \\boldsymbol{\\lambda})} = \\log{p(\\boldsymbol{\\epsilon})} + \\log{\\left|\\det\\left(\\frac{d\\boldsymbol{\\epsilon}}{d\\mathbf{z}}\\right)\\right|} = \\log{p(f(\\mathbf{z}, \\boldsymbol{\\lambda}))} + \\log{\\left|\\det\\left(\\frac{\\partial f(\\mathbf{z}, \\boldsymbol{\\lambda})}{\\partial \\mathbf{z}}\\right)\\right|}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM27aYJAQpc0"
   },
   "source": [
    "Here we define convolutional encoder and decoder. You could use this architecture or experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpbLyoW3d575"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "        )\n",
    "        conv_out_dim = input_shape[1] // 8 * input_shape[2] // 8 * 256\n",
    "        self.fc = nn.Linear(conv_out_dim, 2 * n_latent)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convs(x)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        mu, log_std = self.fc(out).chunk(2, dim=1)\n",
    "        return mu, log_std\n",
    "        \n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, n_latent, output_shape):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.base_size = (128, output_shape[1] // 8, output_shape[2] // 8)\n",
    "        self.fc = nn.Linear(n_latent, np.prod(self.base_size))\n",
    "        self.deconvs = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, output_shape[0], 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.shape[0], *self.base_size)\n",
    "        return self.deconvs(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urpqsQdpeL6A"
   },
   "source": [
    "For autoregressive prior we will use MADE model. Do not change these classes, but read it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFbVp-ZZQuyC"
   },
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    # do not change this class\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)\n",
    "\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    # do not change this class\n",
    "    def __init__(self, input_shape, d, hidden_size=[512, 512]):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.nin = np.prod(input_shape)\n",
    "        self.nout = self.nin * d\n",
    "        self.d = d\n",
    "        self.hidden_sizes = hidden_size\n",
    "        self.ordering = np.arange(self.nin)\n",
    "\n",
    "        self.net = []\n",
    "        hs = [self.nin] + self.hidden_sizes + [self.nout]\n",
    "        for h0, h1 in zip(hs, hs[1:]):\n",
    "            self.net.extend([\n",
    "                MaskedLinear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self.net.pop()\n",
    "        self.net = nn.ModuleList(self.net)\n",
    "\n",
    "        self.m = {}\n",
    "        self.create_mask()\n",
    "\n",
    "    def create_mask(self):\n",
    "        L = len(self.hidden_sizes)\n",
    "\n",
    "        self.m[-1] = self.ordering\n",
    "        for l in range(L):\n",
    "            self.m[l] = np.random.randint(self.m[l - 1].min(),\n",
    "                                          self.nin - 1, size=self.hidden_sizes[l])\n",
    "\n",
    "        masks = [self.m[l - 1][:, None] <= self.m[l][None, :] for l in range(L)]\n",
    "        masks.append(self.m[L - 1][:, None] < self.m[-1][None, :])\n",
    "\n",
    "        masks[-1] = np.repeat(masks[-1], self.d, axis=1)\n",
    "\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "        for l, m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        out = x.view(batch_size, self.nin)\n",
    "        for layer in self.net:\n",
    "            out = layer(out)\n",
    "        out = out.view(batch_size, self.nin, self.d)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6c7rAk1Q0iq"
   },
   "source": [
    "Let implement VAE with autoregressive prior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3AxLZam9cb-"
   },
   "outputs": [],
   "source": [
    "class ARFPriorVAE(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # define made model, encoder and decoder\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def loss(self, x):\n",
    "        x = 2 * x.float() - 1\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder to x to get variational posterior distribution parameters\n",
    "        # 2) sample z from variational posterior distribution (reparametrization trick)\n",
    "        # 3) apply decoder to get reconstruction\n",
    "\n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) compute reconstruction loss (first term in ELBO) \n",
    "        # in this case we could use mse loss \n",
    "        # (we will get beta-VAE model since the contributions of reconstruction loss and KL term become dishonest) \n",
    "        # 2) compute encoder log prob (it is a log of normal distribution on z)\n",
    "        # 3) apply MADE model to z to get mu and log_std\n",
    "\n",
    "        # ====\n",
    "\n",
    "        # this trick is just for model stability (do not touch it)\n",
    "        log_std = torch.tanh(log_std)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) scale z to sigma and shift to mu get epsilon\n",
    "        # 2) compute prior log prob (log of standart normal)\n",
    "        # 3) kl loss is difference between encoder log prob and prior log prob\n",
    "\n",
    "        # ====\n",
    "        return {\n",
    "            'total_loss': recon_loss + kl_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "    \n",
    "    def prior(self, n, use_cuda=True):\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standard normal for prior)\n",
    "\n",
    "        # ====\n",
    "        if use_cuda:\n",
    "            z = z.cuda()\n",
    "        return z\n",
    "\n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            z = self.prior(n)\n",
    "            # investigate how to sample from autoregressive model (do not change this part)\n",
    "            for i in range(self.n_latent):\n",
    "                mu, log_std = self.made(z)[:, i].chunk(2, dim=-1)\n",
    "                log_std = torch.tanh(log_std)\n",
    "                mu, log_std = mu.squeeze(-1), log_std.squeeze(-1)\n",
    "                z[:, i] = (z[:, i] - mu) * torch.exp(-log_std)\n",
    "            return self.decoder(z).cpu().numpy() * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318,
     "referenced_widgets": [
      "55adfcb831924c99bb069853d8397a3b",
      "2647a585de0b4ad19ff5f7ffdb2ba8f1",
      "894ee364e0b740259ec169bfa32c50b9",
      "0b79bde4c4c747e2abd7395bcfd38ea7",
      "b1abff6b813749b49264639ce7fdd84a",
      "e1afda550fed4c80b5a11d8380ce52bd",
      "f3720bd1f7824232be9cec662bba313f",
      "35b2d584b4bd4788a80514f8af9ed246",
      "842aab94c73746ceb924b2af6def1db6",
      "e54dcc30083f4491b99c329cb9767cdf",
      "b587263bff6a4da3aee7caa6d3fe529d"
     ]
    },
    "id": "JcMArVCA9ckD",
    "outputId": "73e7ec50-e651-484f-97e7-4035f480785d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE =  # any adequate value\n",
    "EPOCHS =      # < 20\n",
    "LR =          # < 1e-3\n",
    "# ====\n",
    "\n",
    "model = ARFPriorVAE((3, 32, 32), 16)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, use_cuda=USE_CUDA, epochs=EPOCHS, use_tqdm=True, lr=LR)\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "beJCpjz0edsW",
    "outputId": "13af7da9-f4d8-48d3-b5b0-db9672965397"
   },
   "outputs": [],
   "source": [
    "samples = model.sample(100)\n",
    "\n",
    "x = next(iter(test_loader))[:50]\n",
    "if USE_CUDA:\n",
    "    x = x.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encoder(2 * x - 1)[0]\n",
    "    x_recon = torch.clamp(model.decoder(z) * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 32, 32)\n",
    "reconstructions = reconstructions.cpu()\n",
    "\n",
    "x = next(iter(test_loader))[:20]\n",
    "if USE_CUDA:\n",
    "    x = x.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = 2 * x - 1\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in torch.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps * 0.5 + 0.5, 0, 1)\n",
    "interps = interps.cpu()\n",
    "\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "    \n",
    "show_samples(samples, title='Samples')\n",
    "show_samples(reconstructions, title='Reconstructions')\n",
    "show_samples(interps, title='Interpolations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63RG758ezCrN"
   },
   "source": [
    "**Note:** we do not use complicated architectures for encoder and decoder in this task. That is why your samples could be blurry, it is ok. The main goal of this task is to understand the theory around flow-based prior. But of course you could experiment with more complex networks (like ResNet) for additional points."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw4_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a72ddd90c004cb7a3f0c646beccc0f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b79bde4c4c747e2abd7395bcfd38ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e54dcc30083f4491b99c329cb9767cdf",
      "placeholder": "​",
      "style": "IPY_MODEL_b587263bff6a4da3aee7caa6d3fe529d",
      "value": " 10/10 [05:51&lt;00:00, 35.08s/it]"
     }
    },
    "0f7285aa986840f6a5073f6534eb2947": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "153399d43c2b4830951d3bf285d06f70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "205f8fd5704f4750a1fdf33d1cedd554": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f7285aa986840f6a5073f6534eb2947",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e137db2f7b546e9a327b9f34969fa38",
      "value": 3
     }
    },
    "2647a585de0b4ad19ff5f7ffdb2ba8f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1afda550fed4c80b5a11d8380ce52bd",
      "placeholder": "​",
      "style": "IPY_MODEL_f3720bd1f7824232be9cec662bba313f",
      "value": "100%"
     }
    },
    "35b2d584b4bd4788a80514f8af9ed246": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55adfcb831924c99bb069853d8397a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2647a585de0b4ad19ff5f7ffdb2ba8f1",
       "IPY_MODEL_894ee364e0b740259ec169bfa32c50b9",
       "IPY_MODEL_0b79bde4c4c747e2abd7395bcfd38ea7"
      ],
      "layout": "IPY_MODEL_b1abff6b813749b49264639ce7fdd84a"
     }
    },
    "5e137db2f7b546e9a327b9f34969fa38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64d325abc5c545e1920ec04548bb8290": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b01cb1e3afa14947bb1702bfa401cf15",
      "placeholder": "​",
      "style": "IPY_MODEL_153399d43c2b4830951d3bf285d06f70",
      "value": "100%"
     }
    },
    "81198526b98c42d793428a4d14401039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64d325abc5c545e1920ec04548bb8290",
       "IPY_MODEL_205f8fd5704f4750a1fdf33d1cedd554",
       "IPY_MODEL_ea966af7e6764aa8bd4c7e1c9c338632"
      ],
      "layout": "IPY_MODEL_0a72ddd90c004cb7a3f0c646beccc0f3"
     }
    },
    "842aab94c73746ceb924b2af6def1db6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "894ee364e0b740259ec169bfa32c50b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35b2d584b4bd4788a80514f8af9ed246",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_842aab94c73746ceb924b2af6def1db6",
      "value": 10
     }
    },
    "b01cb1e3afa14947bb1702bfa401cf15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1abff6b813749b49264639ce7fdd84a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b587263bff6a4da3aee7caa6d3fe529d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d61cc7e765d6485d85647d25ef4d8117": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1afda550fed4c80b5a11d8380ce52bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e54dcc30083f4491b99c329cb9767cdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e75f80b28069407aa75b170fe863dc0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea966af7e6764aa8bd4c7e1c9c338632": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e75f80b28069407aa75b170fe863dc0b",
      "placeholder": "​",
      "style": "IPY_MODEL_d61cc7e765d6485d85647d25ef4d8117",
      "value": " 3/3 [03:37&lt;00:00, 72.62s/it]"
     }
    },
    "f3720bd1f7824232be9cec662bba313f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

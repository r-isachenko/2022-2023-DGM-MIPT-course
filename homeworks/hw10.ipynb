{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGM0FvDvR8Gm"
   },
   "source": [
    "# Homework6: Evaluation of implicit models + discrete VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6rgsxFuR8Go"
   },
   "source": [
    "## Task 1: Theory (4pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mg5oRBoOWloW"
   },
   "source": [
    "### Problem 1: Neural ODE vs backprop (2pt)\n",
    "\n",
    "At Lecture 12 we have discussed [Neural ODE](https://arxiv.org/pdf/1806.07366.pdf) model. There we used the adjoint state functions\n",
    "$$\n",
    "\t\\mathbf{a}_{\\mathbf{z}}(t) = \\frac{\\partial L(\\mathbf{y})}{\\partial \\mathbf{z}(t)}; \\quad \\mathbf{a}_{\\boldsymbol{\\theta}}(t) = \\frac{\\partial L(\\mathbf{y})}{\\partial \\boldsymbol{\\theta}(t)}.\n",
    "$$\n",
    "\n",
    "These two functions allowed to derive continuous version of backpropagation algorithm.\n",
    "\n",
    "The formulas for the method are given by Pontryagin theorem. It claims that\n",
    "$$\n",
    "\t\\frac{d \\mathbf{a}_{\\mathbf{z}}(t)}{dt} = - \\mathbf{a}_{\\mathbf{z}}(t)^T \\cdot \\frac{\\partial f(\\mathbf{z}(t), t, \\boldsymbol{\\theta})}{\\partial \\mathbf{z}}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "    \\frac{d \\mathbf{a}_{\\boldsymbol{\\theta}}(t)}{dt} = - \\mathbf{a}_{\\mathbf{z}}(t)^T \\cdot \\frac{\\partial f(\\mathbf{z}(t), t, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}.\n",
    "$$\n",
    "\n",
    "Your task here is to prove the first formula for $\\frac{d \\mathbf{a}_{\\mathbf{z}}(t)}{dt}$.\n",
    "\n",
    "**Hints**: you have to use 3 facts\n",
    "\n",
    "1. Notion of the limit:\n",
    "$$\n",
    "    \\frac{d \\mathbf{a}_{\\mathbf{z}}(t)}{dt} = \\lim_{\\varepsilon \\rightarrow +0} \\frac{\\mathbf{a}_{\\mathbf{z}}(t + \\varepsilon) - \\mathbf{a}_{\\mathbf{z}}(t)}{\\varepsilon}.\n",
    "$$\n",
    "\n",
    "2. Chain rule:\n",
    "$$\n",
    "    \\frac{\\partial L(\\mathbf{y})}{\\partial \\mathbf{z}(t)} = \\frac{\\partial L(\\mathbf{y})}{\\partial \\mathbf{z}(t + \\varepsilon)} \\cdot \\frac{\\mathbf{z}(t + \\varepsilon)}{\\partial \\mathbf{z}(t)}.\n",
    "$$\n",
    "3. Tailor series for \n",
    "$$\n",
    "    \\mathbf{z}(t + \\varepsilon) = \\int_{t}^{t + \\varepsilon} f(\\mathbf{z}(t), t, \\boldsymbol{\\theta}) d t + \\mathbf{z}(t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwS2oTkaVLct"
   },
   "source": [
    "```your solution```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhBBlyW-R8Gp"
   },
   "source": [
    "### Problem 2: Gumbel-Max trick (2pt)\n",
    "\n",
    "In this problem you have to prove the Gumbel-Max trick that we have discussed on the Lecture 12. \n",
    "\n",
    "Let $\\pi_1, \\pi_2, \\dots \\pi_K, \\in (0, 1)$ and $\\sum\\limits_{k = 1}^{K} \\pi_k = 1$. Consider the discrete random variable:\n",
    "\n",
    "$$\n",
    "  c = \\arg\\max_{k} \\left[\\log \\pi_k + g_k\\right].\n",
    "$$\n",
    "\n",
    "In the formula above $g_k$ ($k \\in \\{1, \\dots K\\}$) are independent random variables distributed following the $\\text{Gumbel}(0, 1)$ distribution ([wiki](https://en.wikipedia.org/wiki/Gumbel_distribution)), i.e. $g_k \\sim \\text{Gumbel}(0, 1)$.\n",
    "\n",
    "Note that $g_k = - \\log (- \\log u)$, where $u \\sim \\text{Uniform}[0, 1]$.\n",
    "\n",
    "Our goal is to prove that $c \\sim \\text{Categorical}(\\pi_1, \\dots \\pi_K)$.\n",
    "\n",
    "1. Find cumulative distribution function ($F_{g}(x) = P(g < x)$) of Gumbel distribution.\n",
    "\n",
    "2. Find density of the Gumbel distribution (derivative of cdf).\n",
    "\n",
    "3. Consider random variables $\\zeta_k = \\log \\pi_k + g_k$. Let's fix $k^* \\in \\{1, \\dots K\\}$ and look at the following probability $P\\bigl( \\{\\zeta_{k} \\leq \\zeta_{k^*}\\} \\text{ for all } k \\neq k^*\\bigr)$. Prove that \n",
    "\n",
    "$$\n",
    "  P\\bigl( \\bigcap\\limits_{k \\neq k^*} \\{\\zeta_{k} \\leq \\zeta_{k^*}\\}\\bigr) = \\pi_{k^*}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb8f7MlqR8Gq"
   },
   "source": [
    "```your solution```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJQYtKgA5ate",
    "outputId": "9b6a1c58-289b-4fd4-e6f4-2fa54e8ac591"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "\n",
    "REPO_NAME = \"2022-2023-DGM-MIPT-course\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/r-isachenko/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\n",
    "!pip install ./{REPO_NAME}/homeworks/\n",
    "!mv ./{REPO_NAME}/homeworks/stylegan.py ./stylegan.py\n",
    "!mv ./{REPO_NAME}/homeworks/inception.py ./inception.py\n",
    "!rm -Rf {REPO_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sl59dviN5ekr"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, show_samples, plot_training_curves\n",
    "from dgm_utils import visualize_images, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCNBV60b8CHX",
    "outputId": "509bcb44-f609-400f-e18c-7d8e5e37b391"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "from inception import InceptionV3\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "def reset_seed():\n",
    "    OUTPUT_SEED = 0xBADBEEF\n",
    "    torch.manual_seed(OUTPUT_SEED)\n",
    "    np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "print(\"cuda is available:\", USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8zInnwKIakS"
   },
   "source": [
    "## Task 2: Inception Score and FID (4pt)\n",
    "\n",
    "Here our goal is to understand how to evaluate likelihood-free models using [Inception Score](https://arxiv.org/pdf/1606.03498.pdf) and [Frechet Inception Distance](https://arxiv.org/pdf/1706.08500.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vbQYt2CR8Gy"
   },
   "outputs": [],
   "source": [
    "# do not change this function\n",
    "def plot_losses(losses: np.ndarray, title: str) -> None:\n",
    "    n_itr = len(losses)\n",
    "    xs = np.arange(n_itr)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(xs, losses)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(\"Iterations\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# this is a helper function that we will use further\n",
    "def resize_tensor(x: torch.Tensor, image_size: int) -> torch.Tensor:\n",
    "    return F.interpolate(\n",
    "        x, size=(image_size, image_size), mode=\"bilinear\", align_corners=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEAAQV3SR8Gy"
   },
   "source": [
    "Your task is to implement the *Inception score* and *FID* score and estimate the quality of two trained *StyleGAN* models we have discussed on Seminar 11 and Seminar 12: \n",
    "\n",
    "1. `stylegan_wgangp` is a *StyleGAN* model trained with *WGAN-GP* loss on CIFAR10 dataset ([ckpt_link](https://drive.google.com/file/d/1bTDbmleLXowuGcahsoSBeihSVbGgW52X/view?usp=sharing))\n",
    "\n",
    "2. `stylegan_r1` is a *StyleGAN* model trained with standard gan loss with $R_1$ regularization on CIFAR10 dataset ([ckpt_link](https://drive.google.com/file/d/1PNeESbetxazQkBJbBnoizyWgGKJwfpW5/view?usp=sharing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9y9Zz3qhR8Gy",
    "outputId": "6ba7ac85-d0a4-47e5-d000-243741b2550f"
   },
   "outputs": [],
   "source": [
    "# loading models checkpoints\n",
    "!gdown --id 1bTDbmleLXowuGcahsoSBeihSVbGgW52X\n",
    "!gdown --id 1PNeESbetxazQkBJbBnoizyWgGKJwfpW5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHBUk3T6R8Gz",
    "outputId": "b7eecb5e-7c08-4ec1-a055-f8b04c06117b"
   },
   "outputs": [],
   "source": [
    "from stylegan import MicroStyleGANGenerator\n",
    "from copy import deepcopy\n",
    "\n",
    "sg_wgangp_name = \"stylegan_wgangp_loss_FINAL.pth\"\n",
    "sg_gan_r1_name = \"stylegan_gan_r1_loss_FINAL.pth\"\n",
    "\n",
    "\n",
    "stylegan_wgangp = MicroStyleGANGenerator(\n",
    "    z_dim=128,\n",
    "    map_hidden_dim=256,\n",
    "    w_dim=64,\n",
    "    in_chan=64,\n",
    "    out_chan=3,\n",
    "    kernel_size=3,\n",
    "    hidden_chan=32,\n",
    ")\n",
    "\n",
    "stylegan_r1 = deepcopy(stylegan_wgangp)\n",
    "\n",
    "stylegan_wgangp.load_state_dict(\n",
    "    torch.load(\"./{}\".format(sg_wgangp_name), map_location=\"cpu\")[\"generator\"]\n",
    ")\n",
    "\n",
    "stylegan_r1.load_state_dict(\n",
    "    torch.load(\"./{}\".format(sg_gan_r1_name), map_location=\"cpu\")[\"generator\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3KeHxGER8Gz"
   },
   "source": [
    "Let's look at model samples from `stylegan_r1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "96-IWkZFR8Gz",
    "outputId": "933be8bd-01c3-4004-d22a-cdba137278a6"
   },
   "outputs": [],
   "source": [
    "batch = stylegan_r1.sample(100).detach().cpu().numpy()\n",
    "show_samples(batch, \"CIFAR10 samples, shape = ({0}, {0})\".format(32), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcPqd-c_R8Gz"
   },
   "source": [
    "Let's look at model samples from `stylegan_wgangp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "PA-6CFXDR8Gz",
    "outputId": "6acff8bd-b8f8-430e-cdae-555c06ac4e7a"
   },
   "outputs": [],
   "source": [
    "batch = stylegan_wgangp.sample(100).detach().cpu().numpy()\n",
    "show_samples(batch, \"CIFAR10 samples, shape = ({0}, {0})\".format(32), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuYAc_QOdCHm"
   },
   "source": [
    "###  Inception Score\n",
    "\n",
    "The formula for Inception Score is\n",
    "$$\n",
    "    \\text{IS} = \\exp \\bigl( \\mathbb{E}_{\\mathbf{x}} KL(p(y | \\mathbf{x}) || p(y)) \\bigr),\n",
    "$$\n",
    "\n",
    "where \n",
    "* $p(y | \\mathbf{x})$ is a pretrained classification model with labels $y$ (we will use [Inception V3 model](https://pytorch.org/vision/main/generated/torchvision.models.inception_v3.html));\n",
    "* $p(y) = \\int p(y | \\mathbf{x}) p(\\mathbf{x}) d \\mathbf{x}$ is a marginal distribution on labels.\n",
    "\n",
    "In order to calculate the **Inception** score we will use `InceptionV3` last layer activations (those before computing `Softmax`). The dimensionality of these activations is $1008$.\n",
    "\n",
    "Let initialize our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c54fca68b9d6468a9c78435c4de7a586",
      "d2a32344bf5a4932b77efa38a172f208",
      "fb15111f564941648393028079264f85",
      "c04b8d22571e43c689553a67a2b4a545",
      "a7134794cc924738b0b20d9c072c1363",
      "3fb23f8b1de44f93a28bb1b5f3387835",
      "eaf460bed5804f81a8d8b717ee8baf0e",
      "38a0363819b348a18507f0f3aab7e3c8",
      "4aa629409d124d55820e2ecd269f2f32",
      "1bafb195a18240529f08717ef0645987",
      "95f557727bfe485db72e4c212e155dde"
     ]
    },
    "id": "s1Z2SZO1R8Gz",
    "outputId": "3d610f46-1edd-4378-9731-902b5f556400"
   },
   "outputs": [],
   "source": [
    "DIMS = 1008\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[DIMS]\n",
    "inception_model_act5 = InceptionV3([block_idx])\n",
    "if USE_CUDA:\n",
    "    inception_model_act5 = inception_model_act5.cuda()\n",
    "inception_model_act5.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ah28AGFDYTiO"
   },
   "source": [
    "We need to get class probabilities from our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLxqxX7FUkoQ"
   },
   "outputs": [],
   "source": [
    "def get_inception_probs(x: torch.Tensor, model: object) -> np.ndarray:\n",
    "    # ====\n",
    "    # your code\n",
    "    # apply model and get probs (apply softmax)\n",
    "\n",
    "    # ====\n",
    "    return probs.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def test_get_inception_probs():\n",
    "    x = torch.zeros(size=(1, 3, 10, 10))\n",
    "    if USE_CUDA:\n",
    "        x = x.cuda()\n",
    "    probs = get_inception_probs(x, inception_model_act5)\n",
    "    true_probs = np.array(\n",
    "        [\n",
    "            0.00012616384,\n",
    "            0.00031305864,\n",
    "            0.00019984621,\n",
    "            0.00024997862,\n",
    "            0.00005619833,\n",
    "            0.00010180601,\n",
    "            0.00002303111,\n",
    "            0.0001946776,\n",
    "            0.0015921608,\n",
    "            0.000064336535,\n",
    "        ]\n",
    "    )\n",
    "    assert np.allclose(probs[0, :10], true_probs)\n",
    "\n",
    "\n",
    "test_get_inception_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4VLjo2bUw7X"
   },
   "outputs": [],
   "source": [
    "# this is a helper function that generates samples from the StyleGAN generator\n",
    "def generate_fake_images_stylegan(\n",
    "    sg_generator: object, n_samples: int, batch_size: int\n",
    ") -> np.ndarray:\n",
    "    fake_images = []\n",
    "    for i in range(n_samples // batch_size):\n",
    "        fake_samples = sg_generator.sample(batch_size).cpu().detach().numpy()\n",
    "        fake_images.extend(fake_samples)\n",
    "\n",
    "    fake_samples = sg_generator.sample(n_samples % batch_size).cpu().detach().numpy()\n",
    "    fake_images.extend(fake_samples)\n",
    "    return np.array(fake_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUSPy2WoZsbT"
   },
   "source": [
    "It is the main function for getting Inception Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0M5UpWFAhUm"
   },
   "outputs": [],
   "source": [
    "def get_inception_score(\n",
    "    generator: object,\n",
    "    inception_model: object,\n",
    "    n_samples: int,\n",
    "    batch_size: int = 32,\n",
    "    splits: int = 10,\n",
    ") -> np.ndarray:\n",
    "    if USE_CUDA:\n",
    "        generator = generator.cuda()\n",
    "        inception_model = inception_model.cuda()\n",
    "\n",
    "    generator.eval()\n",
    "    inception_model.eval()\n",
    "\n",
    "    fake_images = generate_fake_images_stylegan(generator, n_samples, batch_size)\n",
    "    loader = torch.utils.data.DataLoader(fake_images, batch_size=batch_size)\n",
    "\n",
    "    # ====\n",
    "    # your code\n",
    "    # get probs of size [n_samples x 1000] for the fake_samples\n",
    "\n",
    "    probs = \n",
    "    # ====\n",
    "\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = probs[k * (n_samples // splits) : (k + 1) * (n_samples // splits), :]\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) calculate p_y mean value of the current part\n",
    "        # 2) calculate KL (use could you entropy function from scipy)\n",
    "        # 3) exponentiate it\n",
    "\n",
    "        split_score = \n",
    "        # ====\n",
    "        split_scores.append(split_score)\n",
    "\n",
    "    return np.mean(split_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OLoHwyBR8G0"
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "BATCH_SIZE = 16\n",
    "SPLITS = 5\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "IS_stylegan_r1 = get_inception_score(\n",
    "    generator=stylegan_r1,\n",
    "    inception_model=inception_model_act5,\n",
    "    n_samples=N_SAMPLES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    splits=SPLITS,\n",
    ")\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "IS_stylegan_wgangp = get_inception_score(\n",
    "    generator=stylegan_wgangp,\n",
    "    inception_model=inception_model_act5,\n",
    "    n_samples=N_SAMPLES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    splits=SPLITS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ9ER2naR8G0"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(IS_stylegan_r1, 6.566, atol=0.1)\n",
    "assert np.allclose(IS_stylegan_wgangp, 6.63, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v11TTvLUR8G0"
   },
   "source": [
    "**In case you have free time**: You are free to evaluate the models from the previous homework via Inception-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nLE_jWLR8G0"
   },
   "outputs": [],
   "source": [
    "# Inception scores of your nice models which beat StyleGAN\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyE1p6t6dHEW"
   },
   "source": [
    "###  Frechet Inception Distance\n",
    "\n",
    "Now we will implement Frechet Inception Distance:\n",
    "\n",
    "$$\n",
    "\t\\text{FID} (\\pi, p) = \\| \\mathbf{m}_{\\pi} - \\mathbf{m}_{p}\\|_2^2 + \\text{Tr} \\left( \\boldsymbol{\\Sigma}_{\\pi} + \\boldsymbol{\\Sigma}_p - 2 \\sqrt{\\boldsymbol{\\Sigma}_{\\pi} \\boldsymbol{\\Sigma}_p} \\right)\n",
    "$$\n",
    "\n",
    "* Representations are the outputs of the intermediate layer from the pretrained classification model (we will use the activations of the last by one layer of `InceptionV3` (which have dimensionality $(2048, 1, 1)$), that's why the last two dimensions should be dropped before FID statistics calculation).\n",
    "* $\\mathbf{m}_{\\pi}$, $\\boldsymbol{\\Sigma}_{\\pi} $ are the mean vector and the covariance matrix of feature representations for samples from $\\pi(\\mathbf{x})$\n",
    "* $\\mathbf{m}_{p}$, $\\boldsymbol{\\Sigma}_p$ are the mean vector and the covariance matrix of feature representations for samples from $p(\\mathbf{x} | \\boldsymbol{\\theta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clhEuN09R8G0"
   },
   "source": [
    "Let initialize our classification model which outputs last by one activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiSAIdGOR8G0",
    "outputId": "075ef88c-b6f1-4654-ceff-f6885c45bbe5"
   },
   "outputs": [],
   "source": [
    "DIMS = 2048\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[DIMS]\n",
    "inception_model_act4 = InceptionV3([block_idx])\n",
    "if USE_CUDA:\n",
    "    inception_model_act4 = inception_model_act4.cuda()\n",
    "inception_model_act4.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxalKhEFR8G1"
   },
   "source": [
    "Here we need samples from the ground truth distribution $\\pi(\\mathbf{x})$ (CIFAR10 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "hVcZ1m9HR8G1",
    "outputId": "8698720e-a454-4440-b0e5-32af2b956972"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"cifar10\", flatten=False, binarize=False)\n",
    "visualize_images(train_data, \"CIFAR10 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cafa_rgSgFwH"
   },
   "source": [
    "Let implement function to take square root of matrix (we need it for the formula above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf2pUE9TVjL0"
   },
   "outputs": [],
   "source": [
    "# this is a helper function, do not change\n",
    "def get_matrix_sqrt(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = x.cpu().detach().numpy()\n",
    "    y = scipy.linalg.sqrtm(y)\n",
    "    if not np.isfinite(y).all():\n",
    "        print(\"bad!\")\n",
    "    return torch.Tensor(y.real, device=x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2UhOSjDg2Ha"
   },
   "source": [
    "Not let implement the function to calculate the distance (it is just the formula above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-DJPP_TWVE5"
   },
   "outputs": [],
   "source": [
    "def get_distance(\n",
    "    mu_x: torch.Tensor, mu_y: torch.Tensor, sigma_x: torch.Tensor, sigma_y: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    # ====\n",
    "    # your code\n",
    "\n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_get_distance():\n",
    "    mu_x = torch.ones(3)\n",
    "    mu_y = torch.ones(3) * 10\n",
    "    sigma_x = torch.eye(3) * 5\n",
    "    sigma_y = torch.eye(3) * 3\n",
    "    dist = get_distance(mu_x, mu_y, sigma_x, sigma_y)\n",
    "    assert np.isclose(dist, 243.7621)\n",
    "\n",
    "\n",
    "test_get_distance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW7ixoCbmn5q"
   },
   "source": [
    "Let implement the function which calculate intermediate representations for real and fake samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTZDMAmDWjnO"
   },
   "outputs": [],
   "source": [
    "def get_features(\n",
    "    generator: object,\n",
    "    inception_model: object,\n",
    "    loader: object,\n",
    "    n_samples: int,\n",
    "    batch_size: int,\n",
    ") -> tuple:\n",
    "    if USE_CUDA:\n",
    "        generator = generator.cuda()\n",
    "        inception_model.cuda()\n",
    "\n",
    "    generator.eval()\n",
    "    inception_model.eval()\n",
    "\n",
    "    fake_features_list = []\n",
    "    real_features_list = []\n",
    "    cur_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for real_samples in loader:\n",
    "            # real_samples = resize_tensor(real_samples, image_size)\n",
    "            if USE_CUDA:\n",
    "                real_samples = real_samples.cuda()\n",
    "            # ====\n",
    "            # your code\n",
    "            # get features of real samples\n",
    "            # drow the w and h dimensions of the obtained features\n",
    "            real_features = \n",
    "            # ====\n",
    "            # print(real_features.shape)\n",
    "            real_features_list.append(real_features)\n",
    "\n",
    "            fake_samples = generator.sample(len(real_samples), step=3)\n",
    "            # fake_samples = resize_tensor(fake_samples, image_size)\n",
    "            if USE_CUDA:\n",
    "                fake_samples = fake_samples.cuda()\n",
    "            # ====\n",
    "            # your code\n",
    "            # get features of fake samples\n",
    "            # drop the w and h dimensions of the the obtained features\n",
    "            fake_features = \n",
    "            # ====\n",
    "            fake_features_list.append(fake_features)\n",
    "\n",
    "            cur_samples += len(real_samples)\n",
    "            if cur_samples >= n_samples:\n",
    "                break\n",
    "\n",
    "    fake_features_all = torch.cat(fake_features_list)\n",
    "    real_features_all = torch.cat(real_features_list)\n",
    "    return fake_features_all, real_features_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gvVL70KWk1e"
   },
   "outputs": [],
   "source": [
    "# this is a helper function, do not change\n",
    "def calculate_stats(fake_features: torch.Tensor, real_features: torch.Tensor) -> tuple:\n",
    "    def get_covariance(features):\n",
    "        return torch.Tensor(np.cov(features.detach().numpy(), rowvar=False))\n",
    "\n",
    "    mu_fake = fake_features.mean(0)\n",
    "    mu_real = real_features.mean(0)\n",
    "    sigma_fake = get_covariance(fake_features)\n",
    "    sigma_real = get_covariance(real_features)\n",
    "    return mu_fake, mu_real, sigma_fake, sigma_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfzrV0_mm6Zs"
   },
   "source": [
    "Now we are ready to implement the main function for getting FID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqe142rrWmVe"
   },
   "outputs": [],
   "source": [
    "def get_frechet_inception_distance(\n",
    "    generator: object,\n",
    "    inception_model: object,\n",
    "    loader: object,\n",
    "    n_samples: int,\n",
    "    batch_size: int,\n",
    ") -> torch.Tensor:\n",
    "    # ====\n",
    "    # your code\n",
    "    # 1) get features\n",
    "    # 2) calculate stats\n",
    "    # 3) get distance\n",
    "\n",
    "    # ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4SPWDTYR8G1"
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 10000  # number of samples in the cifar10 test dataset\n",
    "BATCH_SIZE = 16  # samples per iteration\n",
    "\n",
    "gt_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "FID_r1 = get_frechet_inception_distance(\n",
    "    generator=stylegan_r1,\n",
    "    inception_model=inception_model_act4,\n",
    "    loader=gt_loader,\n",
    "    n_samples=N_SAMPLES,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "FID_wgangp = get_frechet_inception_distance(\n",
    "    generator=stylegan_wgangp,\n",
    "    inception_model=inception_model_act4,\n",
    "    loader=gt_loader,\n",
    "    n_samples=N_SAMPLES,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWZJElJhR8G1"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(FID_r1, 48.35, atol=0.2)\n",
    "assert np.allclose(FID_wgangp, 48.4, atol=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HKLdFZMR8G1"
   },
   "source": [
    "**In case you have free time**: You are free to evaluate the models from the previous homework via FID-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxhMYyIaR8G2"
   },
   "outputs": [],
   "source": [
    "# FID scores of your nice models which beat StyleGAN\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raGa3C3tR8G2"
   },
   "source": [
    "## Task 3: VQ-VAE (5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVuKyJiZR8G2"
   },
   "source": [
    "### Training of VQ-VAE model\n",
    "\n",
    "In this part you will train [VQ-VAE](https://arxiv.org/abs/1711.00937) model that we have discussed at Lecture 12 (see also [VQ-VAE-2](https://arxiv.org/abs/1906.00446) paper). \n",
    "\n",
    "We will you MNIST dataset in this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "CUgY9SH_R8G2",
    "outputId": "2034c87f-a24a-4634-bcde-edda8799b474"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQZ-eqJ2R8G2"
   },
   "source": [
    "VQ-VAE model is a VAE model with discrete latent variable.  \n",
    "\n",
    "**Reminder:**\n",
    "* We define  dictionary (word book) space $\\{\\mathbf{e}_k\\}_{k=1}^K$, where $\\mathbf{e}_k \\in \\mathbb{R}^C$, $K$ is the size of the dictionary. \n",
    "* $\\mathbf{z}_e = \\text{NN}_e(\\mathbf{x}, \\boldsymbol{\\phi})$ - continuous output of encoder network.\n",
    "* $\\mathbf{z}_q = \\mathbf{e}_{k^*}$ is a quantized representation, where $k^* = \\text{argmin}_k \\| \\mathbf{z} - \\mathbf{e}_k \\|$. It is simple nearest neighbor look up.\n",
    "* Out deterministic variational posterior:\n",
    "$$\n",
    "  q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}) = \\begin{cases}\n",
    "  1 , \\quad \\text{for } k^* = \\text{argmin}_k \\| \\mathbf{z}_e - \\mathbf{e}_k \\|; \\\\\n",
    "  0, \\quad \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "* Prior distribution is uniform: $p(c) = \\text{Uniform}\\{1, \\dots, K\\}$.\n",
    "* KL divergence between posterior and prior:\n",
    "$$\n",
    "  KL(q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}), p(c)) = \\log K.\n",
    "$$ \n",
    "* ELBO:\n",
    "$$\n",
    "\t\t\\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})  = \\mathbb{E}_{q(c | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{e}_{c} , \\boldsymbol{\\theta}) - \\log K =  \\log p(\\mathbf{x} | \\mathbf{z}_q, \\boldsymbol{\\theta}) - \\log K.\n",
    "$$\n",
    "* Vector quantization is non-differentiable operation. We will use **straight-through** gradient estimator (we will copy gradients from decoder input $\\mathbf{z}_q$ to encoder output $\\mathbf{z}_e$.\n",
    "\n",
    "**Important modifications:**\n",
    "Due to the straight-through gradient estimation of mapping from $\\mathbf{z}_e$ to $\\mathbf{z}_q$, the embeddings $\\mathbf{e} receive no gradients from the ELBO. \n",
    "\n",
    "Therefore, in order to learn the embedding space we add l2 loss (**codebook loss**) to move the embedding vectors $\\mathbf{e}$ towards the encoder outputs $\\mathbf{z}_e$. \n",
    "\n",
    "Finally, since the volume of the embedding space is dimensionless, it can grow arbitrarily if the embeddings $\\mathbf{e}$ do not train as fast as the encoder parameters. To make sure the encoder commits to an embedding and its output does not grow, we add a **commitment loss**.\n",
    "\n",
    "Thus, the total training objective becomes:\n",
    "$$\n",
    "  \\log p(\\mathbf{x}| \\mathbf{z}_q, \\boldsymbol{\\theta}) + \\| \\text{stop_gradient}(\\mathbf{z}_e) - \\mathbf{e}\\|_2^2 + \\| \\mathbf{z}_e - \\text{stop_gradient}(\\mathbf{e})\\|_2.\n",
    "$$\n",
    "\n",
    "Pay attention to the $\\text{stop_gradient}(*)$ operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbdLrL-_R8G2"
   },
   "source": [
    "Our first step is implement vector quantization procedure. It will also calculate two consistency losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMyf05nDR8G2"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings: int = 128, embedding_dim: int = 16, beta: float = 0.25\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def get_code_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = x.shape[:-1]\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) calculate distances from flatten inputs to embeddings\n",
    "        # 2) find nearest embeddings to each input (use argmin op)\n",
    "        \n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = \n",
    "\n",
    "        # ====\n",
    "        encoding_indices = encoding_indices.view(input_shape)\n",
    "        return encoding_indices\n",
    "\n",
    "    def get_quantized(self, encoding_indices: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get embeddgins with appropriate indices\n",
    "        # 2) transform tensor from BHWC to BCHW format\n",
    "        \n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get indices\n",
    "        # 2) get quantized latents\n",
    "        # 3) calculate codebook and commitment loss\n",
    "        #    do not afraid about stop_gradient op\n",
    "        #    (use .detach() method for quantized latents and x)\n",
    "        # 4) final loss is codebook_loss + beta * commitment_loss\n",
    "        \n",
    "        quantized = \n",
    "\n",
    "        loss = \n",
    "        # ====\n",
    "\n",
    "        # Straight-through estimator (think about it!).\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, loss\n",
    "\n",
    "\n",
    "def test_vector_quantizer():\n",
    "    x = torch.zeros((1, 16, 7, 7))\n",
    "    layer = VectorQuantizer()\n",
    "    indices = layer.get_code_indices(x)\n",
    "    assert indices.shape == (1, 7, 7)\n",
    "    quantized = layer.get_quantized(indices)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    quantized, loss = layer(x)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    assert loss.shape == ()\n",
    "\n",
    "\n",
    "test_vector_quantizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264s0fmGR8G2"
   },
   "source": [
    "We will use simple encoder/decoder with several strided convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yOFvfbDR8G2"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with Conv2d and ReLU activation\n",
    "        self.net = \n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with ConvTransposed2d and ReLU activation\n",
    "\n",
    "        self.net = \n",
    "        # ====\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92b4_3vfR8G2"
   },
   "source": [
    "Now we are ready to define our model. It consists of encoder, decoder and vector quatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_6RJDcxR8G2"
   },
   "outputs": [],
   "source": [
    "class VQVAEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ce_loss_scale: float = 1.0,\n",
    "        latent_dim: int = 16,\n",
    "        num_embeddings: int = 64,\n",
    "        latent_size: tuple = (7, 7),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, latent_dim)\n",
    "        self.ce_loss_scale = ce_loss_scale\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) apply vector quantizer (it returns quantized representation + vq_loss)\n",
    "        # 3) apply decoder (it returns decoded samples)\n",
    "\n",
    "        # ====\n",
    "        return decoded, vq_loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply model\n",
    "        # 2) get cross entropy loss\n",
    "\n",
    "\n",
    "        # ====\n",
    "        return {\n",
    "            \"total_loss\": self.ce_loss_scale * ce_loss + vq_loss,\n",
    "            \"ce_loss\": self.ce_loss_scale * ce_loss,\n",
    "            \"vq_loss\": vq_loss,\n",
    "        }\n",
    "\n",
    "    def get_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) get indices of codes using vector quantizer\n",
    "\n",
    "        # ====\n",
    "        return codebook_indices\n",
    "\n",
    "    def prior(self, n: int) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # prior distribution is uniform\n",
    "        # 1) get samples from categorical distribution\n",
    "        # 2) get quantized representations using vector quantizer\n",
    "\n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def sample_from_logits(self, logits: torch.Tensor) -> np.ndarray:\n",
    "        # ====\n",
    "        # your code\n",
    "        # our model will return logits, this method applies softmax and samples from the distribution\n",
    "        # 1) apply softmax to the logits\n",
    "        # 2) sample from the distribution (e.x. you could use torch.multinomial)\n",
    "        # be careful with the sizes of the tensors (may be you need to permute/reshape dimensios)\n",
    "\n",
    "        # ====\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) sample from prior distribution\n",
    "            # 2) apply decoder\n",
    "            # 3) sample from logits\n",
    "\n",
    "            # ====\n",
    "            return samples\n",
    "\n",
    "\n",
    "def test_vqvae_model():\n",
    "    model = VQVAEModel().cuda()\n",
    "    x = torch.zeros((2, 1, 28, 28)).cuda()\n",
    "\n",
    "    encoded = model.encoder(x)\n",
    "    size = encoded.shape[2:]\n",
    "    assert size == model.latent_size\n",
    "\n",
    "    indices = model.get_indices(x)\n",
    "    assert indices.shape == (2, 7, 7)\n",
    "\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "\n",
    "    quantized = model.prior(10)\n",
    "    assert quantized.shape == (10, 16, *model.latent_size)\n",
    "\n",
    "    decoded = model.decoder(quantized)\n",
    "    assert decoded.shape == (10, 2, 28, 28)\n",
    "\n",
    "    sampled = model.sample(10)\n",
    "    assert sampled.shape == (10, 1, 28, 28)\n",
    "\n",
    "\n",
    "test_vqvae_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4j-1Uy-R8G2"
   },
   "source": [
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318,
     "referenced_widgets": [
      "e0cb92fb74bb401695628934fd7c4e4d",
      "0efb97a6f31e4bddb0c7dfebb824a0ff",
      "0299829eea9d4d4cb823477e8cbb6018",
      "8cf54bd2c2b44aa3a7730b482afd8e64",
      "a9ea3202ddd54e74bde5cde486aea630",
      "e384e8a41377438baa9390cf7aa3af57",
      "ee35d1342b034398ab1bb15b0144875b",
      "cd5c5727163945d89fdfd334b35abc65",
      "9647df5eb21d4a5dac6045d1963888ca",
      "e253e2bf17734ea68faeee04184dfc5d",
      "d9accea6deca4fc9946f994dcd5e1132"
     ]
    },
    "id": "DIPgCoCgR8G2",
    "outputId": "dd470734-53bf-42fc-d5db-2a5ccc19d93c"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE =   # any adequate value\n",
    "EPOCHS =       # < 30\n",
    "LR =           # < 1e-2\n",
    "CE_SCALE =     # 0.01 < x < 30.0\n",
    "# ====\n",
    "\n",
    "train_data, test_data = load_pickle(\"/content/mnist.pkl\", flatten=False, binarize=True)\n",
    "\n",
    "model = VQVAEModel(ce_loss_scale=CE_SCALE, latent_dim=16, num_embeddings=128)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_losses, test_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    use_cuda=USE_CUDA,\n",
    "    use_tqdm=True,\n",
    "    lr=LR,\n",
    ")\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75RLttZHR8G3"
   },
   "source": [
    "Now we is able to sample from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "QGeLaBCwR8G3",
    "outputId": "a481b2f0-dab6-49aa-e4c6-9942f526e8f8"
   },
   "outputs": [],
   "source": [
    "# Test losses\n",
    "for key, value in test_losses.items():\n",
    "    print(\"{}: {:.4f}\".format(key, value[-1]))\n",
    "\n",
    "# Samples\n",
    "samples = model.sample(100)\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")\n",
    "\n",
    "# Reconstructions\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    decoded, _ = model(x)\n",
    "    x_recon = model.sample_from_logits(decoded)\n",
    "x = x.cpu().numpy()\n",
    "reconstructions = np.concatenate((x, x_recon), axis=0)\n",
    "reconstructions = reconstructions.astype(\"float32\")\n",
    "show_samples(reconstructions, title=\"Reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFR_E8mER8G3"
   },
   "source": [
    "Probably you will get bad samples :(\n",
    "\n",
    "Do not worry, may be it is OK, we will try to fix your samples! Make sure that reconstructions are almost perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1p3iNP3MR8G3"
   },
   "source": [
    "Here, we will visualize latent code indices for test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    },
    "id": "mbYZlnW4R8G3",
    "outputId": "8f300007-1f22-4b59-db46-360612616cbb"
   },
   "outputs": [],
   "source": [
    "test_images = next(iter(test_loader))[:100]\n",
    "x = test_images.cuda()\n",
    "codebook_indices = model.get_indices(x).cpu().unsqueeze(1)\n",
    "\n",
    "show_samples(test_images, \"Test images\")\n",
    "show_samples(codebook_indices, \"Test codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZiyXXO1R8G3"
   },
   "source": [
    "### Training of prior autoregressive model\n",
    "\n",
    "The samples from our VQ-VAE model is not good enough. The authors of the original VQ-VAE paper proposed to train autoregressive model in the latent space after we trained VQ-VAE model.\n",
    "\n",
    "Remember we have discussed **ELBO surgery** and **aggregrated posterior**. Let recall what do we have in VAE:\n",
    "* **Training:** we get latent variables $\\mathbf{z}$ from variational posterior $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})$ for every object $\\mathbf{x}$ and then applies decoder ($p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$). It means that in average decoder is applied to the latent variables from aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "* **Inference:** We apply decoder to the latent variables from prior distribution $p(\\mathbf{z})$.\n",
    "\n",
    "It means that if our aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$ and prior $p(\\mathbf{z})$ is too far from each other, then we get inconsistency.\n",
    "\n",
    "So let train to remove this inconsistency. To be concrete, let train (autoregressive) model in the latent space that will try to predict samples from the aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "\n",
    "We will use our good friend: PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Btu_EK5R8G3"
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, mask_type: str, in_channels: int, out_channels: int, kernel_size: int = 5\n",
    "    ) -> None:\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "\n",
    "    def create_mask(self, mask_type: str) -> None:\n",
    "        # ====\n",
    "        # your code\n",
    "        # do not forget about mask_type\n",
    "\n",
    "        # ====\n",
    "\n",
    "\n",
    "def test_masked_conv2d():\n",
    "    layer = MaskedConv2d(\"A\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.zeros((2, 2)))\n",
    "\n",
    "    layer = MaskedConv2d(\"B\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.ones((2, 2)))\n",
    "\n",
    "\n",
    "test_masked_conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxKYRKjQR8G3"
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int = 128,\n",
    "        input_shape: tuple = (7, 7),\n",
    "        n_filters: int = 32,\n",
    "        kernel_size: int = 5,\n",
    "        n_layers: int = 5,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply the sequence of MaskedConv2d -> ReLU\n",
    "        # the last layer should be MaskedConv2d (not ReLU)\n",
    "        # Note 1: the first conv layer should be of type 'A'\n",
    "        # Note 2: final output_dim in MaskedConv2d must be 2\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # read the forward method carefully\n",
    "        flattened = x.view((-1, 1))\n",
    "        encodings = torch.zeros(flattened.shape[0], self.num_embeddings).cuda()\n",
    "        encodings.scatter_(1, flattened, 1)\n",
    "        encodings = encodings.view((-1, *self.input_shape, self.num_embeddings))\n",
    "        encodings = encodings.permute((0, 3, 1, 2))\n",
    "        out = self.net(encodings)\n",
    "        out = out.view(-1, self.num_embeddings, 1, *self.input_shape)\n",
    "        return out\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "\n",
    "        # ====\n",
    "        return {\"total_loss\": total_loss}\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        # read carefully the sampling process\n",
    "        samples = torch.zeros(n, 1, *self.input_shape, dtype=torch.int64).cuda()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[0]):\n",
    "                for c in range(self.input_shape[1]):\n",
    "                    logits = self(samples)[:, :, :, r, c]\n",
    "                    probs = F.softmax(logits, dim=1).squeeze(-1)\n",
    "                    samples[:, 0, r, c] = torch.multinomial(\n",
    "                        probs, num_samples=1\n",
    "                    ).squeeze(-1)\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "\n",
    "def test_pixelcnn():\n",
    "    model = PixelCNN().cuda()\n",
    "    x = torch.zeros((1, 1, 7, 7), dtype=torch.int64).cuda()\n",
    "    output = model(x)\n",
    "    assert output.shape == (1, 128, 1, 7, 7)\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "    samples = model.sample(10)\n",
    "    assert samples.shape == (10, 1, 7, 7)\n",
    "\n",
    "\n",
    "test_pixelcnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FscrJfPjR8G3"
   },
   "source": [
    "Now we need to get our train and test samples. Our model will predict indices of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8YXckIjR8G3"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (7, 7)  # input shape of your latent space\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# you have to get indices of the emdeddings from the VQ-VAE model for train and test data\n",
    "\n",
    "# ====\n",
    "\n",
    "assert isinstance(train_indices, np.ndarray)\n",
    "assert isinstance(test_indices, np.ndarray)\n",
    "assert train_indices.shape == (60000, 1, *INPUT_SHAPE)\n",
    "assert test_indices.shape == (10000, 1, *INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a2201bb3559e464fbd741e542389eb23",
      "b6c06f2bab5645c78466b9fa341bab1e",
      "e6185d0954de43bf98554d1838efe6b8",
      "41b5e41936b743cd8f8b9d735303aca1",
      "6a7ee05ad9184e82bb4e4ca75e9b9722",
      "fe54ce5a872c441aae0f352926f0c94d",
      "e60d59a011e346f692b120108b3a4af4",
      "922bc85f5241419d9d46b926b2e4f9fc",
      "dee5df1ffc9c45928719059395e9739b",
      "1af3d11caeb54a7aa886b83834637a80",
      "85dd73f62db94135ae2ccf6d8c07d163"
     ]
    },
    "id": "r967dypsR8G3",
    "outputId": "a0f8321a-9608-4acf-baf5-da08301f5e07"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters by your own\n",
    "EPOCHS =       # > 5\n",
    "BATCH_SIZE =   # any adequate value\n",
    "LR =           # < 1e-2\n",
    "N_LAYERS =     # < 10\n",
    "N_FILTERS =    # < 128\n",
    "# ====\n",
    "\n",
    "prior_model = PixelCNN(\n",
    "    input_shape=INPUT_SHAPE, n_filters=N_FILTERS, kernel_size=5, n_layers=N_LAYERS\n",
    ")\n",
    "\n",
    "train_loader = data.DataLoader(train_indices, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_indices, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(\n",
    "    prior_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    use_tqdm=True,\n",
    "    use_cuda=USE_CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_CeMZVkR8G3"
   },
   "source": [
    "Now we are ready to sample from our VQ-VAE model. The difference here that we will sample our embedding indices from the PixelCNN prior model instead of the Uniform prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "lS1MfS8dR8G3",
    "outputId": "dea86f8c-5d2b-4fff-bb55-b8e18f4683ff"
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 100\n",
    "indices = prior_model.sample(N_SAMPLES).squeeze(1)\n",
    "quantized = model.vq_layer.get_quantized(torch.Tensor(indices).int().cuda())\n",
    "logits = model.decoder(quantized)\n",
    "samples = model.sample_from_logits(logits)\n",
    "\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onSbThJtR8G3"
   },
   "source": [
    "Here you have to get samples with good enough quality!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0299829eea9d4d4cb823477e8cbb6018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd5c5727163945d89fdfd334b35abc65",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9647df5eb21d4a5dac6045d1963888ca",
      "value": 10
     }
    },
    "0efb97a6f31e4bddb0c7dfebb824a0ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e384e8a41377438baa9390cf7aa3af57",
      "placeholder": "",
      "style": "IPY_MODEL_ee35d1342b034398ab1bb15b0144875b",
      "value": "100%"
     }
    },
    "1af3d11caeb54a7aa886b83834637a80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bafb195a18240529f08717ef0645987": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38a0363819b348a18507f0f3aab7e3c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fb23f8b1de44f93a28bb1b5f3387835": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41b5e41936b743cd8f8b9d735303aca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1af3d11caeb54a7aa886b83834637a80",
      "placeholder": "",
      "style": "IPY_MODEL_85dd73f62db94135ae2ccf6d8c07d163",
      "value": " 5/5 [02:27&lt;00:00, 29.69s/it]"
     }
    },
    "4aa629409d124d55820e2ecd269f2f32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a7ee05ad9184e82bb4e4ca75e9b9722": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85dd73f62db94135ae2ccf6d8c07d163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8cf54bd2c2b44aa3a7730b482afd8e64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e253e2bf17734ea68faeee04184dfc5d",
      "placeholder": "",
      "style": "IPY_MODEL_d9accea6deca4fc9946f994dcd5e1132",
      "value": " 10/10 [02:57&lt;00:00, 18.29s/it]"
     }
    },
    "922bc85f5241419d9d46b926b2e4f9fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95f557727bfe485db72e4c212e155dde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9647df5eb21d4a5dac6045d1963888ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2201bb3559e464fbd741e542389eb23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6c06f2bab5645c78466b9fa341bab1e",
       "IPY_MODEL_e6185d0954de43bf98554d1838efe6b8",
       "IPY_MODEL_41b5e41936b743cd8f8b9d735303aca1"
      ],
      "layout": "IPY_MODEL_6a7ee05ad9184e82bb4e4ca75e9b9722"
     }
    },
    "a7134794cc924738b0b20d9c072c1363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9ea3202ddd54e74bde5cde486aea630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6c06f2bab5645c78466b9fa341bab1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe54ce5a872c441aae0f352926f0c94d",
      "placeholder": "",
      "style": "IPY_MODEL_e60d59a011e346f692b120108b3a4af4",
      "value": "100%"
     }
    },
    "c04b8d22571e43c689553a67a2b4a545": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bafb195a18240529f08717ef0645987",
      "placeholder": "",
      "style": "IPY_MODEL_95f557727bfe485db72e4c212e155dde",
      "value": " 91.2M/91.2M [00:00&lt;00:00, 187MB/s]"
     }
    },
    "c54fca68b9d6468a9c78435c4de7a586": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d2a32344bf5a4932b77efa38a172f208",
       "IPY_MODEL_fb15111f564941648393028079264f85",
       "IPY_MODEL_c04b8d22571e43c689553a67a2b4a545"
      ],
      "layout": "IPY_MODEL_a7134794cc924738b0b20d9c072c1363"
     }
    },
    "cd5c5727163945d89fdfd334b35abc65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2a32344bf5a4932b77efa38a172f208": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fb23f8b1de44f93a28bb1b5f3387835",
      "placeholder": "",
      "style": "IPY_MODEL_eaf460bed5804f81a8d8b717ee8baf0e",
      "value": "100%"
     }
    },
    "d9accea6deca4fc9946f994dcd5e1132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dee5df1ffc9c45928719059395e9739b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e0cb92fb74bb401695628934fd7c4e4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0efb97a6f31e4bddb0c7dfebb824a0ff",
       "IPY_MODEL_0299829eea9d4d4cb823477e8cbb6018",
       "IPY_MODEL_8cf54bd2c2b44aa3a7730b482afd8e64"
      ],
      "layout": "IPY_MODEL_a9ea3202ddd54e74bde5cde486aea630"
     }
    },
    "e253e2bf17734ea68faeee04184dfc5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e384e8a41377438baa9390cf7aa3af57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e60d59a011e346f692b120108b3a4af4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6185d0954de43bf98554d1838efe6b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_922bc85f5241419d9d46b926b2e4f9fc",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dee5df1ffc9c45928719059395e9739b",
      "value": 5
     }
    },
    "eaf460bed5804f81a8d8b717ee8baf0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee35d1342b034398ab1bb15b0144875b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb15111f564941648393028079264f85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38a0363819b348a18507f0f3aab7e3c8",
      "max": 95628359,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4aa629409d124d55820e2ecd269f2f32",
      "value": 95628359
     }
    },
    "fe54ce5a872c441aae0f352926f0c94d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

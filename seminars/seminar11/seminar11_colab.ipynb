{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f48b05-23ab-4662-a247-b5519ac17611",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/seminar11_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opening-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outdoor-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Tdata\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as TD\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as TVdatasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../homeworks') # to grab dgm_utils from ../../homeworks directory\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    GPU_DEVICE = 2\n",
    "    torch.cuda.set_device(GPU_DEVICE)\n",
    "else:\n",
    "    DEVICE='cpu'\n",
    "# DEVICE='cpu'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# dgm_utils\n",
    "from dgm_utils import train_model, show_samples, visualize_images\n",
    "from dgm_utils import visualize_2d_samples, visualize_2d_densities, visualize_2d_data\n",
    "\n",
    "def reset_seed():\n",
    "    OUTPUT_SEED = 0xBADBEEF\n",
    "    torch.manual_seed(OUTPUT_SEED)\n",
    "    np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b85505-f773-4119-a16e-ac67c7adf8b1",
   "metadata": {},
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 11</center>\n",
    "\n",
    "<center><img src=\"https://github.com/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/pics/mipt_logo.png?raw=true\" width=600 /></center>\n",
    "<center>28.03.2023</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ceff32-7713-4ed6-bbc6-28c5d97c0d33",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "1. StyleGAN. End discussion and evaluation\n",
    "     \n",
    "2. Unpaired image-to-image translation\n",
    "\n",
    "    - CycleGAN details and coding for colored MNIST -> USPS\n",
    "\n",
    "<!-- 3. Beyond GANs: Neural Optimal Transport \n",
    "\n",
    "    - Theory and coding for 2D and coding for colored MNIST -> USPS -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9e021-7e9a-4518-b96f-a25bc3cf043b",
   "metadata": {},
   "source": [
    "## StyleGAN. End discussion and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a3503-e724-47cf-856d-ac5b27647fa9",
   "metadata": {},
   "source": [
    "## Unpaired image-to-image translation problem\n",
    "\n",
    "We have two domains of data:\n",
    "* Source domain $X$, data distribution is $\\mathbb{P}$\n",
    "* Target domain $Y$, data distribution is $\\mathbb{Q}$\n",
    "\n",
    "<center><img src=\"https://github.com/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/pics/paired_unpaired_i2i.png?raw=true\" width=600 /></center>\n",
    "<!-- <center><img src=\"pics/paired_unpaired_i2i.png\" width=600 /></center> -->\n",
    "\n",
    "**Unpaired I2I examples**:\n",
    "\n",
    "<center><img src=\"https://github.com/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/pics/unpaired_i2i_examples.png?raw=true\" width=1000 /></center>\n",
    "<!-- <center><img src=\"pics/unpaired_i2i_examples.png\" width=1000 /></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd3b2b-cb7c-4478-9fdf-9effc1b2711a",
   "metadata": {},
   "source": [
    "**Question**: Why can't we just solve the *unpaired image-to-image problem* via GANs, treating the latent distribution of GAN generator as $\\mathbb{P}$ (source domain $X$), and adjusting the generated distribution of GAN generator to be $\\mathbb{Q}$ (target domain $Y$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954877aa-ab3e-4e0b-93a4-d62cfa649f20",
   "metadata": {},
   "source": [
    "### CycleGAN\n",
    "\n",
    "link to the paper: [Zhu et. al.](https://arxiv.org/pdf/1703.10593.pdf).\n",
    "\n",
    "project page: [link](https://junyanz.github.io/CycleGAN/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88795e1b-a0d6-4dfb-8ddb-1354f745598f",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/pics/cyclegan_scheme.png?raw=true\" width=1000 /></center>\n",
    "<!-- <center><img src=\"pics/cyclegan_scheme.png\" width=1000 /></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d0b87-3de8-48fd-bbc5-b68c04b9972c",
   "metadata": {},
   "source": [
    "* Seminal work, a lot of GAN-based Unpaired I2I approaches are based on CycleGAN. ($\\sim$ 16K citations)\n",
    "\n",
    "* Authors propose to utilize two generators:\n",
    "$$G_{XY}: X \\rightarrow Y \\, , \\, \\text{transforms source domain to target domain}\\\\\n",
    "G_{YX}: Y \\rightarrow X \\, , \\, \\text{transforms target domain to source domain},$$\n",
    "    and discriminators $D_Y$ and $D_X$, which imposes the target distribution $\\mathbb{Q}$ and source distribution $\\mathbb{P}$ constraings corresponingly.\n",
    "    \n",
    "* **Cycle Consistency loss**:\n",
    "\n",
    "<center><img src=\"https://github.com/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/pics/cycle_consistency_loss.png?raw=true\" width=800 /></center>\n",
    "<!-- <center><img src=\"pics/cycle_consistency_loss.png\" width=800 /></center> -->\n",
    "\n",
    "* Optional **Identity** loss:\n",
    "\n",
    "    <center><img src=\"https://github.com/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/pics/cyclegan_id_loss.png?raw=true\" width=670 /></center>\n",
    "\n",
    "    Citation from the article:\n",
    "> \\[Identity loss\\] regularize the generator to be near an identity mapping when real samples of the target domain are provided as the input to the generator.\n",
    "\n",
    "* Final loss (to be **minimized by $G_{XY}, G_{YX}$** and **maximized/minimized** (depends on actual $\\mathcal{L}_{\\text{GAN}}$) by **$D_Y, D_X$**:\n",
    "$$\\mathcal{L}(G_{XY}, G_{YX}, D_{Y}, D_{X}) = \\mathcal{L}_{\\text{GAN}}(G_{XY}, D_{Y}) + \\mathcal{L}_{\\text{GAN}}(G_{YX}, D_{X}) + \\lambda_{\\text{cyc}}\\mathcal{L}_{\\text{cyc}}(G_{XY}, G_{YX}) + \\lambda_{\\text{id}} \\mathcal{L}_{\\text{id}}(G_{XY}, G_{YX})$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f235659-26df-4201-a868-84ba93c03e47",
   "metadata": {},
   "source": [
    "<!-- **Question.** What is the final objective? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e6065-a9ab-42ea-9b96-201121fafd8f",
   "metadata": {},
   "source": [
    "<!-- $$\n",
    "G^*_{XY}, G_{YX}^* = \\arg\\min\\limits_{G_{XY}, G_{YX}} \\max\\limits_{D_X, D_Y} \\mathcal{L}(G_{XY}, G_{YX}, D_{Y}, D_{X})\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6349edc-77a1-4d35-8ab1-d4b62138b52c",
   "metadata": {},
   "source": [
    "### CycleGAN practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072bd044-2e56-44ca-91c1-cea5c76d659a",
   "metadata": {},
   "source": [
    "**Disclaimer**: code for dataset loading and networks is taken from the [NOT_github](https://github.com/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_strong_solutions.ipynb). The implementation of `CycleGAN` is based on [CycleGAN_github](https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/cyclegan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8709aa-cfa5-47c3-a1e2-27beb3ea8340",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "We consider $16\\times 16$ colorized **MNIST** & **USPS** digit datasets and learn a map between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296be985-7b07-4641-88fc-43f5202b993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def random_color(im):\n",
    "    hue = 360*np.random.rand()\n",
    "    d = (im *(hue%60)/60)\n",
    "    im_min, im_inc, im_dec = torch.zeros_like(im), d, im - d\n",
    "    c_im = torch.zeros((3, im.shape[1], im.shape[2]))\n",
    "    H = round(hue/60) % 6    \n",
    "    cmap = [[0, 3, 2], [2, 0, 3], [1, 0, 3], [1, 2, 0], [3, 1, 0], [0, 1, 2]]\n",
    "    return torch.cat((im, im_min, im_dec, im_inc), dim=0)[cmap[H]]\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize(16),\n",
    "    transforms.ToTensor(),\n",
    "    random_color,\n",
    "    transforms.Normalize([0.5],[0.5])\n",
    "])\n",
    "\n",
    "# Load train datasets\n",
    "mnist_train = TVdatasets.MNIST(root='./data', train=True, download=True, transform=TRANSFORM)\n",
    "usps_train = TVdatasets.USPS(root='./data', train=True, download=True, transform=TRANSFORM)\n",
    "\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "usps_loader = torch.utils.data.DataLoader(usps_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# Load test datasets\n",
    "mnist_test = TVdatasets.MNIST(root='./data', train=False, download=True, transform=TRANSFORM)\n",
    "usps_test = TVdatasets.USPS(root='./data', train=False, download=True, transform=TRANSFORM)\n",
    "\n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=BATCH_SIZE)\n",
    "usps_test_loader = torch.utils.data.DataLoader(usps_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "# We use only a few samples from them for the qualitative analysis\n",
    "X_test_fixed = next(iter(mnist_test_loader))[0]\n",
    "Y_test_fixed = next(iter(usps_test_loader))[0]\n",
    "\n",
    "del mnist_test_loader, usps_test_loader, mnist_test, usps_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bb0c43-ef0d-4f6c-8e90-ed61483d341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_grid(grid, figsize=(10, 10), title=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title)\n",
    "    plt.imshow(np.asarray(grid.permute(1, 2, 0).cpu()))\n",
    "\n",
    "draw_grid(\n",
    "    make_grid(next(iter(mnist_loader))[0][:10], nrow=10, normalize=True), \n",
    "    title=r'random MNIST $16\\times 16$ samples')\n",
    "draw_grid(\n",
    "    make_grid(next(iter(usps_loader))[0][:10], nrow=10, normalize=True), \n",
    "    title=r'random USPS $16\\times 16$ samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f40132-3ce8-4a32-a0c4-984fc65e79f0",
   "metadata": {},
   "source": [
    "Infinite samplers from the **MNIST** and **USPS**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1982dbf3-12fd-4731-9f57-1138d26bdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_mnist, iter_usps = iter(mnist_loader), iter(usps_loader)\n",
    "\n",
    "def sample_mnist():\n",
    "    global iter_mnist, mnist_loader\n",
    "    try:\n",
    "        return next(iter_mnist)[0]\n",
    "    except StopIteration:\n",
    "        iter_mnist = iter(mnist_loader)\n",
    "        return next(iter_mnist)[0]\n",
    "\n",
    "def sample_usps():\n",
    "    global iter_usps, usps_loader\n",
    "    try:\n",
    "        return next(iter_usps)[0]\n",
    "    except StopIteration:\n",
    "        iter_usps = iter(usps_loader)\n",
    "        return next(iter_usps)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ad1e1-bf02-4419-8072-36d36c90b832",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "**Generators**. Original architecture utilize ResNet-like architecture. We will use something more simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ac9fa9b-9809-4089-be6f-2a801b110084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G_XY (G_YX) parameters: 2067971\n"
     ]
    }
   ],
   "source": [
    "# Generator from domain X to domain Y\n",
    "\n",
    "G_XY = nn.Sequential(\n",
    "    ### YOUR CODE HERE\n",
    "    # Stack several Conv2d with nonlinearities. \n",
    "    # Note, that dimensionality of the input should be the same as dimensionality of output (3, 16, 16)\n",
    ")\n",
    "\n",
    "# Generator from domain Y to domain X\n",
    "\n",
    "G_YX = deepcopy(G_XY)\n",
    "\n",
    "G_XY = G_XY.to(DEVICE)\n",
    "G_YX = G_YX.to(DEVICE)\n",
    "\n",
    "print('G_XY (G_YX) parameters:', np.sum([np.prod(p.shape) for p in G_XY.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80430a45-d034-4d63-83d2-14bbc9923fa5",
   "metadata": {},
   "source": [
    "**Discriminators**.\n",
    "\n",
    "The idea is inspired by `PatchGAN`: [Isola et. al.](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "* Citation from the original article: \n",
    "> This discriminator tries to classify if each $N \\times N$ patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of $D$ [discriminator].\n",
    "\n",
    "<center><img src=\"https://github.com/r-isachenko/2022-2023-DGM-MIPT-course/blob/main/seminars/seminar11/pics/PatchGAN_discriminator.png?raw=true\" width=600 /></center>\n",
    "<!-- <center><img src=\"pics/PatchGAN_discriminator.png\" width=600 /></center> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb88c273-6f58-4595-b287-d9610f4da2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_X (D_Y) parameters: 2628993\n"
     ]
    }
   ],
   "source": [
    "# Discriminator for domain X\n",
    "\n",
    "class SimplePatchDiscriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ### YOUR CODE HERE\n",
    "            # Define fully-confolutional NN.\n",
    "            # Use Conv2d with stride > 1\n",
    "            # Use LeakyReLU nonlinearity and stabilize the discriminator with InstanceNorm2d.\n",
    "            # The dimensionality of output should be (batch_size, 1, 2, 2)\n",
    "        )\n",
    "        self.output_shape = (1, 2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Discriminator for domain X\n",
    "\n",
    "D_X = SimplePatchDiscriminator()\n",
    "\n",
    "# Discriminator for domain Y\n",
    "\n",
    "D_Y = SimplePatchDiscriminator()\n",
    "\n",
    "D_X = D_X.to(DEVICE)\n",
    "D_Y = D_Y.to(DEVICE)\n",
    "\n",
    "print('D_X (D_Y) parameters:', np.sum([np.prod(p.shape) for p in D_X.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83009782-a427-42a4-b196-91c2991464e6",
   "metadata": {},
   "source": [
    "Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238ece3f-a61a-41c9-9e79-034cba382e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.026)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "D_Y.apply(weights_init_normal)\n",
    "D_X.apply(weights_init_normal)\n",
    "G_XY.apply(weights_init_normal)\n",
    "G_YX.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120dd6b8-a974-44f2-8eec-b887d4b3b83a",
   "metadata": {},
   "source": [
    "### GAN loss\n",
    "\n",
    "**Question.** Which $\\mathcal{L}_{\\text{GAN}}$ we use in the loss?\n",
    "\n",
    "**Answer**: Generally speaking, we can use any."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707eac14-77b4-43b9-8db1-df99a498b90b",
   "metadata": {},
   "source": [
    "Authors propose to utilize **Least-Squared GAN loss** ([Mao et. al.](https://arxiv.org/pdf/1611.04076.pdf)).\n",
    "\n",
    "#### Computation of $\\mathcal{L}_{\\text{GAN}}(G_{XY}, D_{Y})$\n",
    "\n",
    "* When training the generator $G_{XY}$, we *minimize*: \n",
    "$$\\mathbb{E}_{x \\sim \\mathbb{P}} \\big[D_Y(G_{XY}(x)) - 1\\big]^2 \\, \\rightarrow \\, \\min\\limits_{G_{XY}}$$\n",
    "\n",
    "* When training the discriminator $D_{Y}$ we *minimize*:\n",
    "\n",
    "$$\\mathbb{E}_{y \\sim \\mathbb{Q}} \\big[D_Y(y) - 1\\big]^2 + \\mathbb{E}_{x \\sim \\mathbb{P}} \\big[D_Y(G_{XY}(x))\\big]^2 \\, \\rightarrow \\, \\min\\limits_{D_Y}$$\n",
    "\n",
    "**Note**: Discriminator tries to learn $1$ for real samples and $0$ for fake samples.\n",
    "\n",
    "**Question.** How will these formulas change for $\\mathcal{L}_{\\text{GAN}}(G_{YX}, D_{X})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38dbed-5c0a-4ce7-afae-81440347cbab",
   "metadata": {},
   "source": [
    "### Final training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd7962-40da-4754-815e-83aef5b98097",
   "metadata": {},
   "source": [
    "Let $\\{ X_1, \\dots, X_N\\} = \\overline{X}$ is a mini-batch from $\\mathbb{P}$ (source domain), and $\\{ Y_1, \\dots, Y_N\\} = \\overline{Y}$ is a mini-batch from $\\mathbb{Q}$ (target domain).\n",
    "\n",
    "**CycleGAN loss**:\n",
    "\n",
    "$$\\mathcal{L}(G_{XY}, G_{YX}, D_{Y}, D_{X}) = \\mathcal{L}_{\\text{GAN}}(G_{XY}, D_{Y}) + \\mathcal{L}_{\\text{GAN}}(G_{YX}, D_{X}) + \\lambda_{\\text{cyc}}\\mathcal{L}_{\\text{cyc}}(G_{XY}, G_{YX}) + \\lambda_{\\text{id}} \\mathcal{L}_{\\text{id}}(G_{XY}, G_{YX})$$\n",
    "\n",
    "#### Optimization of generators\n",
    "\n",
    "We optimize the generators $G_{XY}$ and $G_{YX}$ simultaneously:\n",
    "$$\n",
    "\\mathcal{L}(G_{XY}, G_{YX}, D_{Y}, D_{X}) \\, \\rightarrow \\, \\text{ minimize by }G_{XY}, G_{YX}\n",
    "$$\n",
    "\n",
    "**Mini-batch** estimate of the loss:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}_{G}(G_{XY}, G_{YX}) = \\text{Const}(G_{XY}, G_{YX}) + \\color{red}{\\frac{1}{N} \\sum\\limits_{X_i \\in \\overline{X}} \\big[D_Y(G_{XY}(X_i)) - 1\\big]^2 + \\frac{1}{N} \\sum\\limits_{Y_i \\in \\overline{Y}} \\big[D_X(G_{YX}(Y_i)) - 1\\big]^2} + \\\\ \\lambda_{\\text{cyc}}  \\color{magenta}{\\left\\{\\frac{1}{N} \\sum\\limits_{X_i \\in \\overline{X}} \\big[\\Vert G_{YX}(G_{XY}(X_i)) - X_i \\Vert_1\\big]\n",
    "+ \\frac{1}{N} \\sum\\limits_{Y_i \\in \\overline{Y}} \\big[\\Vert G_{XY}(G_{YX}(Y_i)) - Y_i \\Vert_1\\big]\\right\\}} + \\\\\n",
    "+ \\lambda_{\\text{id}}  \\color{blue}{\\left\\{\\frac{1}{N} \\sum\\limits_{X_i \\in \\overline{X}} \\big[\\Vert G_{YX}(X_i) - X_i \\Vert_1\\big]\n",
    "+ \\frac{1}{N} \\sum\\limits_{Y_i \\in \\overline{Y}} \\big[\\Vert G_{XY}(Y_i) - Y_i \\Vert_1\\big]\\right\\}} \\, \\rightarrow \\, \\text{ minimize by }G_{XY}, G_{YX}\n",
    "$$\n",
    "\n",
    "#### Optimization of discriminators\n",
    "\n",
    "We optimize the discriminators separately. Consider $D_Y$:\n",
    "$$\n",
    "\\mathcal{L}(G_{XY}, G_{YX}, D_{Y}, D_{X}) \\, \\rightarrow \\, \\text{ minimize by }D_Y\n",
    "$$\n",
    "\n",
    "**Mini-batch** estimate of the loss:\n",
    "\n",
    "$$\\hat{\\mathcal{L}}_{D_Y}(D_Y) = \\text{Const}(D_Y) + \\frac{1}{N} \\sum\\limits_{Y_i \\in \\overline{Y}} \\big[D_Y(Y_i) - 1\\big]^2 + \\frac{1}{N} \\sum\\limits_{X_i \\in \\overline{X}} \\big[D_Y(G_{XY}(X_i))\\big]^2 \\, \\rightarrow \\, \\text{ minimize by }D_Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20067840-8118-4f4c-ba98-7c627637603e",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ebde252-5034-4949-bcdd-e9116974d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-4 # choose LR\n",
    "BETA1 = 0.5 # choose beta1\n",
    "BETA2 = 0.999 # choose beta2\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_XY.parameters(), G_YX.parameters()), lr=LR, betas=(BETA1, BETA2)\n",
    ")\n",
    "optimizer_D_X = torch.optim.Adam(D_X.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
    "optimizer_D_Y = torch.optim.Adam(D_Y.parameters(), lr=LR, betas=(BETA1, BETA2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7ebc3-1e74-4402-857b-c691f4065c14",
   "metadata": {},
   "source": [
    "#### Replay Buffer\n",
    "\n",
    "* Stabilizes the training.\n",
    "\n",
    "* Used to update the discriminators using a history of generated images rather than the ones produced by the latest generators.\n",
    "\n",
    "In our simple **colored MNIST** -> **USPS** task everything is simple. So, we use just dummy Replay BufferðŸ˜Š.\n",
    "\n",
    "*In case you have free time...* : Implement correct Replay Buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87de1f7b-3a9e-4ac1-8139-186687455411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1000):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9d539-ebbe-46ed-8a4a-7b30d818265b",
   "metadata": {},
   "source": [
    "Util function to demonstrate generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d730881c-2eeb-4efa-bdba-9dcaa21a33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cyclegan_images(step):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    G_XY.eval()\n",
    "    G_YX.eval()\n",
    "    real_X = torch.tensor(X_test_fixed[:8], device=DEVICE)\n",
    "    real_Y = torch.tensor(Y_test_fixed[:8], device=DEVICE)\n",
    "    fake_X = G_YX(real_Y).clip(-1., 1.)\n",
    "    fake_Y = G_XY(real_X).clip(-1., 1.)\n",
    "    # Arange images along x-axis\n",
    "    real_X = make_grid(real_X, nrow=8, normalize=True)\n",
    "    real_Y = make_grid(real_Y, nrow=8, normalize=True)\n",
    "    fake_X = make_grid(fake_X, nrow=8, normalize=True)\n",
    "    fake_Y = make_grid(fake_Y, nrow=8, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_X, fake_Y, real_Y, fake_X), 1)\n",
    "    draw_grid(image_grid, title='Step: {}'.format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24aa7b5f-62df-434f-a754-98bd7009fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cyclegan_images('init')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51326e3e-7cd2-4ea1-978c-6cede3bbf405",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "85d97f32-2221-40f9-8b68-2d684244cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 10000 + 1\n",
    "\n",
    "LAMBDA_CYCLE = 10.0\n",
    "LAMBDA_ID = 5.0\n",
    "visualize_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd628a3-0a3b-417c-8b53-e56bb180b13f",
   "metadata": {},
   "source": [
    "Criteria for training: (**Note**: take a look at `Final training Objectives`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91da0580-47ef-4995-b7f9-de89dabf1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "criterion_GAN = # torch loss function, used when computing GAN loss\n",
    "criterion_cycle = # torch loss function, used when computing cycle consistency loss\n",
    "criterion_identity = # torch loss function, used when computing identity loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9041d6c-422b-4038-91e2-2c8cb2c00653",
   "metadata": {},
   "source": [
    "Replay buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4ca4bdc6-22dd-4a17-bbfc-65e0fa3253f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_X_buffer = ReplayBuffer()\n",
    "fake_Y_buffer = ReplayBuffer()\n",
    "\n",
    "from seminar11_utils import StatsManager\n",
    "SM = StatsManager('D_loss', 'G_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8049f0-4b34-4a92-89e9-b8d0c5862aa3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52884350-9839-447e-82e3-895681032faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in tqdm(range(MAX_STEPS)):\n",
    "\n",
    "    # Set model input\n",
    "    real_X = sample_mnist()\n",
    "    real_X = torch.tensor(real_X, device=DEVICE)\n",
    "\n",
    "    real_Y = sample_usps()\n",
    "    real_Y = torch.tensor(real_Y, device=DEVICE)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    GT_shape = ### YOUR CODE HERE\n",
    "    valid = torch.ones(GT_shape, device=DEVICE)\n",
    "    fake = torch.zeros(GT_shape, device=DEVICE)\n",
    "\n",
    "    ##################\n",
    "    #  Train Generators for both domains X and Y\n",
    "\n",
    "    G_XY.train()\n",
    "    G_YX.train()\n",
    "\n",
    "    optimizer_G.zero_grad()\n",
    "\n",
    "    # Identity loss\n",
    "    loss_id_X = ### YOUR CODE HERE\n",
    "    loss_id_Y = ### YOUR CODE HERE\n",
    "\n",
    "    loss_identity = (loss_id_X + loss_id_Y) / 2\n",
    "\n",
    "    # GAN loss\n",
    "    fake_Y = G_XY(real_X)\n",
    "    loss_GAN_XY = ### YOUR CODE HERE\n",
    "    fake_X = G_YX(real_Y)\n",
    "    loss_GAN_YX = ### YOUR CODE HERE\n",
    "\n",
    "    loss_GAN = (loss_GAN_XY + loss_GAN_YX) / 2\n",
    "\n",
    "    # Cycle loss\n",
    "    recov_X = ### YOUR CODE HERE\n",
    "    loss_cycle_X = ### YOUR CODE HERE\n",
    "    recov_Y = ### YOUR CODE HERE\n",
    "    loss_cycle_Y = ### YOUR CODE HERE\n",
    "\n",
    "    loss_cycle = (loss_cycle_X + loss_cycle_Y) / 2\n",
    "\n",
    "    # Total loss\n",
    "    loss_G = loss_GAN + LAMBDA_CYCLE * loss_cycle + LAMBDA_ID * loss_identity\n",
    "\n",
    "    loss_G.backward()\n",
    "    optimizer_G.step()\n",
    "\n",
    "    SM.upd('G_loss', loss_G.item())\n",
    "\n",
    "    #####################\n",
    "    #  Train Discriminator for domain X\n",
    "\n",
    "    D_X.train()\n",
    "\n",
    "    optimizer_D_X.zero_grad()\n",
    "\n",
    "    # Real loss\n",
    "    loss_real = ### YOUR CODE HERE\n",
    "    # Fake loss (on batch of previously generated samples)\n",
    "    fake_X_ = fake_X_buffer.push_and_pop(fake_X)\n",
    "    loss_fake = ### YOUR CODE HERE\n",
    "    # Total loss\n",
    "    loss_D_X = (loss_real + loss_fake) / 2\n",
    "\n",
    "    loss_D_X.backward()\n",
    "    optimizer_D_X.step()\n",
    "\n",
    "    ####################\n",
    "    #  Train Discriminator for domain Y\n",
    "    \n",
    "    D_Y.train()\n",
    "\n",
    "    optimizer_D_Y.zero_grad()\n",
    "\n",
    "    # Real loss\n",
    "    loss_real = ### YOUR CODE HERE\n",
    "    # Fake loss (on batch of previously generated samples)\n",
    "    fake_Y_ = fake_Y_buffer.push_and_pop(fake_Y)\n",
    "    loss_fake = ### YOUR CODE HERE\n",
    "    # Total loss\n",
    "    loss_D_Y = (loss_real + loss_fake) / 2\n",
    "\n",
    "    loss_D_Y.backward()\n",
    "    optimizer_D_Y.step()\n",
    "\n",
    "    loss_D = (loss_D_X + loss_D_Y) / 2\n",
    "\n",
    "    SM.upd('D_loss', loss_D.item())\n",
    "    \n",
    "    # Visualization\n",
    "    if visualize_steps and step % visualize_steps == 0:\n",
    "        clear_output(wait=True)\n",
    "        print('Step {}'.format(step))\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "        SM.draw(axes)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        show_cyclegan_images(step)\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73ca924c-abe2-4b94-9015-080bc0859c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(\n",
    "#     {\n",
    "#         'G_XY': G_XY.state_dict(),\n",
    "#         'G_YX': G_YX.state_dict(),\n",
    "#     },\n",
    "#     'checkpoints/CycleGAN_mnist2usps_2.pth'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9370aae1-c07a-4de5-afb4-2c4bb245ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('checkpoints/CycleGAN_mnist2usps.pth', map_location=DEVICE)\n",
    "# G_XY.load_state_dict(checkpoint['G_XY'])\n",
    "# G_YX.load_state_dict(checkpoint['G_YX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a274401-e9ca-48e8-ad14-ae99616cce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_cyclegan_images('final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c3594-c064-467e-877e-6ae4f315fdf4",
   "metadata": {},
   "source": [
    "<!-- ## Beyond GANs: Neural Optimal Transport  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0244204-f5ab-4fa3-beae-8325b24c9d69",
   "metadata": {},
   "source": [
    "<!-- The seminar materials are taken from the recent [**Neural Optimal Transport** paper](https://arxiv.org/pdf/2201.12220.pdf) repository [NOT_github](https://github.com/iamalexkorotin/NeuralOptimalTransport) by [Alex Korotin](https://scholar.google.ru/citations?user=1rIIvjAAAAAJ&hl=en)\n",
    "\n",
    "**Storng OT**: <a href=\"https://colab.research.google.com/github/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_strong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Open In Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=Open%20in%20Github&color=lightgrey)](https://github.com/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_strong.ipynb)\n",
    "\n",
    "**Weak OT**:\n",
    "<a href=\"https://colab.research.google.com/github/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_weak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Open In Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=Open%20in%20Github&color=lightgrey)](https://github.com/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_weak.ipynb) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54797c8b-c7fd-41fc-ba88-ce8eb5df1744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opening-serbia",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outdoor-rescue",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as TD\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../homeworks') # to grab dgm_utils from ../../homeworks directory\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    GPU_DEVICE = 2\n",
    "    torch.cuda.set_device(GPU_DEVICE)\n",
    "else:\n",
    "    DEVICE='cpu'\n",
    "# DEVICE='cpu'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# dgm_utils\n",
    "from dgm_utils import train_model, show_samples, visualize_images\n",
    "from dgm_utils import visualize_2d_samples, visualize_2d_densities, visualize_2d_data\n",
    "\n",
    "def reset_seed():\n",
    "    OUTPUT_SEED = 0xBADBEEF\n",
    "    torch.manual_seed(OUTPUT_SEED)\n",
    "    np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b85505-f773-4119-a16e-ac67c7adf8b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 12</center>\n",
    "\n",
    "<center><img src=\"pics/mipt_logo.png\" width=600 /></center>\n",
    "<center>11.04.2023</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ceff32-7713-4ed6-bbc6-28c5d97c0d33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan\n",
    "\n",
    "1. Beyond GANs: Neural Optimal Transport: theory and practice.\n",
    "\n",
    "2. VQ-VAE implementation hints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847752c-1576-4068-905a-37f361310339",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Optimal Transport\n",
    "\n",
    "The seminar materials are taken from the recent [**Neural Optimal Transport** paper](https://arxiv.org/pdf/2201.12220.pdf) repository [NOT_github](https://github.com/iamalexkorotin/NeuralOptimalTransport) by [Alex Korotin](https://scholar.google.ru/citations?user=1rIIvjAAAAAJ&hl=en)\n",
    "\n",
    "**Storng OT**: <a href=\"https://colab.research.google.com/github/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_strong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Open In Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=Open%20in%20Github&color=lightgrey)](https://github.com/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_strong.ipynb)\n",
    "\n",
    "**Strong OT, solutions**:\n",
    "<a href=\"https://colab.research.google.com/github/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_strong_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Open In Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=Open%20in%20Github&color=lightgrey)](https://github.com/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_strong_solutions.ipynb)\n",
    "\n",
    "**Weak OT**:\n",
    "<a href=\"https://colab.research.google.com/github/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_weak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Open In Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=Open%20in%20Github&color=lightgrey)](https://github.com/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_weak.ipynb)\n",
    "\n",
    "**Weak OT, solutions**:\n",
    "<a href=\"https://colab.research.google.com/github/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_weak_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [![Open In Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=Open%20in%20Github&color=lightgrey)](https://github.com/iamalexkorotin/NeuralOptimalTransport/blob/main/seminars/NOT_seminar_weak_solutions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34f3d8-4e1c-430c-a0d9-1b73cac56613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VQ-VAE implementation hints\n",
    "\n",
    "Link to the original paper: [article](https://arxiv.org/pdf/1711.00937v2.pdf).\n",
    "\n",
    "<center><img src=\"pics/vqvae_scheme.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad9936-4ac3-42da-b2a2-55f481190475",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Original VAE ELBO objective**:\n",
    "\n",
    "$$L_{\\text{scaled}}(\\phi, \\theta) = \\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left(\\mathbb{E}_{z_n \\sim q(z_n| x_n, \\phi)} \\ln p(x_n|z_n, \\theta) - KL(q(z| x_n, \\phi)||p(z))\\right)$$\n",
    "\n",
    "**VQ-VAE ELBO objective**: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3703e-9c47-4df7-a6d1-fe2935c419a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$L_{\\text{scaled}}(\\phi, \\theta) = \\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left( \\ln p(x_n|z_q(x_n| \\phi), \\theta)\\right) - \\log K + \\text{<embeddings related loss>}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca1875-1875-4be2-92c3-f5bdf27c2401",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 1**: Let data images have shape `(1, w, h)` (Binarized MNIST). What is **encoder** $z_e(\\cdot | \\phi)$? Input dimensionality? Output dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79fd7f2-0f4c-4495-9f48-36055627ab7e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "# x : tensor (bs, 1, w, h) \n",
    "# D is embedding vectors dimensionality\n",
    "\n",
    "encoded = z_e(x) # (bs, D, w_z, h_z)\n",
    "encoded_perm = encoded.permute(0, 2, 3, 1) # latent codes (bs, w_z, h_z, D)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266e07a-b4be-4960-a53a-05097e461836",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 2**: What is posterior distribution $q(z | x, \\phi)$? How to map the **encoder** output `encoded` to a sample $z \\sim q(z | x, \\phi)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd448a-ebbc-49de-8904-61d5265465b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The distribution $q(z | x, \\phi)$ is $\\big{[}\\text{Categorical}(\\pi_1, \\dots, \\pi_K)\\big{]}^{w\\_z \\,\\times\\, h\\_z}$, i.e. $z \\in \\{1, 2, \\dots K\\}^{w\\_z \\,\\times\\, h\\_z}$\n",
    "\n",
    "    ```python\n",
    "    # encoded : tensor (bs, w_z, h_z, D)\n",
    "    codes = encoded2z(encoded_perm) # (bs, w_z, h_z)\n",
    "    ```\n",
    "\n",
    "* Model the embeddings $e_1, \\dots, e_K$; where $e_i \\in \\mathbb{R}^D$, then:\n",
    "\n",
    "$$q(z_{i, j} = k | x, \\phi) = \\begin{cases}1, k = \\arg\\min\\limits_{k'} \\Vert z_{i, j} - e_{k'} \\Vert \\\\ 0, \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "```python\n",
    "# K : number of embedding vectors\n",
    "# D : dimensionality of embedding vectors \n",
    "embedding_module = torch.nn.Embedding(K, D)\n",
    "...\n",
    "# encoded : (bs, w_z, h_z, D)\n",
    "flat_encoded = encoded_perm.view(-1, D) # (bs * w_z * h_z, D)\n",
    "distances = dist(flat_encoded, embedding_module.weight)  # (bs * w_z * h_z , K)\n",
    "\n",
    "# What to do next? How to transform `distances` to quantized codes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76045d-4192-4535-bde5-7f167db052a1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"pics/fqgan_lookup.png\" width=600 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea809140-989b-4e05-81a5-b431e0d45068",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 3.** How to map `codes` to decoder input $z_q(x | \\phi)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f962d26-9054-43e6-a9c8-9f97450a9b2c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"pics/fqgan_cnn.png\" width=600 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2dddb-f158-4722-aada-5859c1966251",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "quantized_perm = embedding_module(codes) # (bs, w_z, h_z, D)\n",
    "\n",
    "# What to do next to prepare z_q(x)?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68eb03-4655-4b45-8d2e-c7a0dd6e17a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 4** What is **decoder** ? Input dimensionality? Output dimensionality? (Recall: we train our model on Binarized MNIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426e321",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Output: `(bs, 2, 1, w, h)` -> `nn.CrossEntropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac3267-18e7-43bf-9207-398a4e929735",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VQ-VAE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb72df6-2042-4025-8066-bb70c688653f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src=\"pics/vqvae_scheme.png\" width=1000 /></center>\n",
    "\n",
    "**VQ-VAE ELBO**:\n",
    "$$L_{\\text{scaled}}(\\phi, \\theta) = \\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left( \\ln p(x_n|z_q(x_n| \\phi), \\theta)\\right) - \\log K + \\text{<embeddings related loss>}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03dbe14-36f0-4a6d-a453-e2da6fdd05be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 1.** How to estimate $\\frac{\\partial z_q(x_n | \\phi)}{\\partial \\phi}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c224ef9-42fa-4f73-9f59-eecfa1814c89",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"pics/straight_through.png\" width=900 /></center>\n",
    "\n",
    "```python\n",
    "# encoded : (bs, D, w_z, h_z)\n",
    "# quantized : (bs, D, w_z, h_z)\n",
    "quantized = encoded + (quantized - encoded).detach()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c02d8-fa3b-49c1-bd6a-a518293b619b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Embedings related loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4d7fa-077e-4ec0-a4df-e93952cb7562",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$L_{\\text{emb}}(\\zeta, \\phi) = \\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left( \\Vert e_{z_e(x_n)} - \\text{stop_gradient}(z_e(x_n)) \\Vert_2^2 + \\beta \\Vert \\text{stop_gradient}(e_{z_e(x_n)}) - z_e(x_n) \\Vert_2^2 \\right)$$\n",
    "\n",
    "* $\\zeta$ parameterizes the embedding layer (weights of $e_i, \\, i \\in \\{1, \\dots K\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e593f30-e170-4d35-8d91-34d089f06a20",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Final Loss** **(to be maximized!):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001b33e-3775-48ef-8d3e-cc537f73b0ec",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$L_{\\text{scaled}}(\\phi, \\theta) = \\frac{1}{N} \\sum\\limits_{n = 1}^{N} \\left( \\ln p(x_n|z_q(x_n| \\phi), \\theta) - \\Vert e_{z_e(x_n | \\phi)} - \\text{stop_gradient}(z_e(x_n | \\phi)) \\Vert_2^2 - \\beta \\Vert \\text{stop_gradient}(e_{z_e(x_n | \\phi)}) - z_e(x_n | \\phi) \\Vert_2^2\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90868370-1a9b-4787-acde-ea82674128ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VQ-VAE prior\n",
    "\n",
    "<center><img src=\"pics/vqvae_scheme.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6c22e-c692-4cd8-b075-f8e69cc08aca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall the $q_{\\text{agg}}(z)$ distribution: \n",
    "\n",
    "$$q_{\\text{agg}}(z) = \\frac{1}{N} \\sum\\limits_{n = 1}^{N} q(z | x_n) \\, , \\, z \\in \\mathbb{R}^{d_{\\text{latent}}}$$\n",
    "\n",
    "**Question 1**: What is the type of distribution $q_{\\text{agg}}(z)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e794f-4975-4797-b66c-6d18015a1539",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $w\\_z \\,\\times\\, h\\_z$-dimensional K-categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dca034-56cd-4e42-81ae-55ef1ce68b71",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Empirical fact**: it is worth to sample from $q_{\\text{agg}}(z)$ at inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60146888-0062-4bd5-8842-24d2fb8c7ba9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 2**: How to obtain $q_{\\text{agg}}(z)$ given trained **VQ-VAE** model? What NN model can be used for this purpose? What is the data to train the model? What is the objective function for such training? Input dims/output tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf80b5c-27f0-4c89-9ae5-4964b5a649e0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* One choise is **PixelCNN** AR model\n",
    "\n",
    "* The data is all `codes` $q(z | x_n)$\n",
    "\n",
    "* `nn.CrossEntropy` is loss function\n",
    "\n",
    "* Input : \n",
    "    * Input: `(bs, w_z, h_z)`\n",
    "    \n",
    "    * one-hot-encoding: `(bs, K, w_z, h_z)`\n",
    "    \n",
    "    * `PixelCNN` application -> output `(bs, K, w_z, h_z)`\n",
    "    \n",
    "    * CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550af1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "rise": {
   "scroll": true,
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
